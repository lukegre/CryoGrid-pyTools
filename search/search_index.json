{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CryoGrid-pyTools CryoGrid-pyTools is a Python package designed to facilitate working with CryoGrid MATLAB data in Python. It provides tools for reading and processing various types of data including: CryoGrid output (from OUT_regridded_FCI2 ) Simple MATLAB struct files ERA5 forcing data Excel run configuration files Features Easy-to-use interface for reading CryoGrid output files Support for both single and multiple file reading Conversion of MATLAB data structures to Python Integration with common data science libraries like xarray Quick Links Installation Guide Getting Started API Reference License This project is licensed under the MIT License - see the LICENSE file for details.","title":"Home"},{"location":"#cryogrid-pytools","text":"CryoGrid-pyTools is a Python package designed to facilitate working with CryoGrid MATLAB data in Python. It provides tools for reading and processing various types of data including: CryoGrid output (from OUT_regridded_FCI2 ) Simple MATLAB struct files ERA5 forcing data Excel run configuration files","title":"CryoGrid-pyTools"},{"location":"#features","text":"Easy-to-use interface for reading CryoGrid output files Support for both single and multiple file reading Conversion of MATLAB data structures to Python Integration with common data science libraries like xarray","title":"Features"},{"location":"#quick-links","text":"Installation Guide Getting Started API Reference","title":"Quick Links"},{"location":"#license","text":"This project is licensed under the MIT License - see the LICENSE file for details.","title":"License"},{"location":"api-reference/","text":"API Reference This page contains the detailed API reference for CryoGrid-pyTools. Main Functions cryogrid_pytools.read_OUT_regridded_file(fname, deepest_point=None) Read a CryoGrid OUT_regridded[_FCI2] file and return it as an xarray dataset. Parameters fname : str Path to the .mat file deepest_point : float, optional Represents the deepest depth of the profile relative to the surfface. If not provided, then elevation is returned. Negative values represent depths below the surface. Returns ds : xarray.Dataset Dataset with dimensions 'time' and 'level'. The elevation coordinate represents the elevation above sea level based on the DEM. If deepest_point is provided, then an additional coordinate, depth , represents depth above surface. Notes For plotting, use `ds['variable'].plot(y='depth'/'elevation'). Source code in cryogrid_pytools/outputs.py def read_OUT_regridded_file(fname: str, deepest_point=None) -> xr.Dataset: \"\"\" Read a CryoGrid OUT_regridded[_FCI2] file and return it as an xarray dataset. Parameters ---------- fname : str Path to the .mat file deepest_point : float, optional Represents the deepest depth of the profile relative to the surfface. If not provided, then elevation is returned. Negative values represent depths below the surface. Returns ------- ds : xarray.Dataset Dataset with dimensions 'time' and 'level'. The `elevation` coordinate represents the elevation above sea level based on the DEM. If deepest_point is provided, then an additional coordinate, `depth`, represents depth above surface. Notes ----- For plotting, use `ds['variable'].plot(y='depth'/'elevation'). \"\"\" from cryogrid_pytools.matlab_helpers import ( matlab2datetime, read_mat_struct_flat_as_dict, ) dat = read_mat_struct_flat_as_dict(fname) for key in dat: dat[key] = dat[key].squeeze() ds = xr.Dataset() ds.attrs[\"filename\"] = fname times = matlab2datetime(dat.pop(\"timestamp\")) elev = dat.pop(\"depths\") for key in dat: ds[key] = xr.DataArray( data=dat[key].astype(\"float32\"), dims=[\"level\", \"time\"], coords={\"time\": times}, ) ds = ds.chunk(dict(time=-1)) ds[\"elevation\"] = xr.DataArray( data=elev, dims=[\"level\"], attrs={\"units\": \"m\", \"long_name\": \"Elevation above sea level\"}, ) ds = ds.set_coords(\"elevation\") if deepest_point is not None: dz = deepest_point - ds[\"elevation\"].min() ds[\"depth\"] = (ds[\"elevation\"] + dz).assign_attrs( units=\"m\", long_name=\"Depth relative to surface\" ) ds = ds.set_coords(\"depth\") ds = ds.chunk(dict(time=-1)) return ds cryogrid_pytools.read_OUT_regridded_clusters(fname_glob, deepest_point, gridcell_func=lambda fname: fname.split('_')[-2], **joblib_kwargs) Reads multiple files that are put out by the OUT_regridded class (and _FCI2) Parameters fname_glob: str Path of the files that you want to read in. Use same notation as for glob(). Note that it expects name to follow the format some_project_name_GRIDCELL_ID_date.mat where GRIDCELL_ID will be extracted to assign the gridcell dimension. These GRIDCELL_IDs correspond with the index of the data in the flattened array. deepest_point: float When setting the configuration for when the data should be saved, the maximum depth is set. Give this number as a negative number here. joblib_kwargs: dict Uses the joblib library to do parallel reading of the files. Defaults are: n_jobs=-1, backend='threading', verbose=1 Returns xr.Dataset An array with dimensions gridcell, depth, time. Variables depend on how the class was configured, but elevation will also be a variable. Source code in cryogrid_pytools/outputs.py def read_OUT_regridded_clusters( fname_glob: str, deepest_point: float, gridcell_func=lambda fname: fname.split(\"_\")[-2], **joblib_kwargs, ) -> xr.Dataset: \"\"\" Reads multiple files that are put out by the OUT_regridded class (and _FCI2) Parameters ---------- fname_glob: str Path of the files that you want to read in. Use same notation as for glob(). Note that it expects name to follow the format `some_project_name_GRIDCELL_ID_date.mat` where GRIDCELL_ID will be extracted to assign the gridcell dimension. These GRIDCELL_IDs correspond with the index of the data in the flattened array. deepest_point: float When setting the configuration for when the data should be saved, the maximum depth is set. Give this number as a negative number here. joblib_kwargs: dict Uses the joblib library to do parallel reading of the files. Defaults are: n_jobs=-1, backend='threading', verbose=1 Returns ------- xr.Dataset An array with dimensions gridcell, depth, time. Variables depend on how the class was configured, but elevation will also be a variable. \"\"\" import inspect from .utils import regex_glob # get the file list flist = regex_glob(fname_glob) # extract the gridcell from the file name gridcell = [gridcell_func(f) for f in flist] digits = [g.isdigit() for g in gridcell] if len(flist) == 0: raise FileNotFoundError(f\"No files found with {fname_glob}\") elif len(gridcell) != len(flist): raise ValueError(f\"Could not extract gridcell from file names for {fname_glob}\") elif not all(digits): not_digit = np.unique([f for f, d in zip(flist, digits) if not d]) bad_func = \"\".join(inspect.getsource(gridcell_func).split(\"lambda\")[1:]).strip() raise ValueError( f\"Check your fname_glob ({fname_glob}) \\n or gridcell_func ({bad_func}). \\nGridcell ID \" f\"not a number for the following files:\\n{not_digit}\" ) else: gridcell = [int(g) for g in gridcell] list_of_ds = _read_OUT_regridded_parallel(flist, deepest_point, **joblib_kwargs) # assign the gridcell dimension so that we can combine the data by coordinates and time list_of_ds = [ds.expand_dims(gridcell=[c]) for ds, c in zip(list_of_ds, gridcell)] ds = xr.combine_by_coords(list_of_ds, combine_attrs=\"drop_conflicts\") assert isinstance(ds, xr.Dataset), \"Something went wrong with the parallel reading.\" # transpose data so that plotting is quick and easy ds = ds.transpose(\"gridcell\", \"level\", \"time\", ...) return ds cryogrid_pytools.matlab_helpers.read_mat_struct_flat_as_dict(fname, key=None) Read a MATLAB struct from a .mat file and return it as a dictionary. Assumes that the struct is flat, i.e. it does not contain any nested structs. Parameters fname : str Path to the .mat file key : str, optional The name of the matlab key in the .mat file. If None is passed [default], then the first key that does not start with an underscore is used. If a string is passed, then the corresponding key is used. Returns data : dict Dictionary with the struct fields as keys and the corresponding data as values. Source code in cryogrid_pytools/matlab_helpers.py def read_mat_struct_flat_as_dict(fname: str, key=None) -> dict: \"\"\" Read a MATLAB struct from a .mat file and return it as a dictionary. Assumes that the struct is flat, i.e. it does not contain any nested structs. Parameters ---------- fname : str Path to the .mat file key : str, optional The name of the matlab key in the .mat file. If None is passed [default], then the first key that does not start with an underscore is used. If a string is passed, then the corresponding key is used. Returns ------- data : dict Dictionary with the struct fields as keys and the corresponding data as values. \"\"\" from scipy.io import loadmat raw = loadmat(fname) keys = [k for k in raw.keys() if not k.startswith(\"_\")] if key is None: logger.log( 5, f\"No key specified. Using first key that does not start with an underscore: {keys[0]}\", ) key = keys[0] elif key not in keys: raise ValueError( f\"Key '{key}' not found in .mat file. Available keys are: {keys}\" ) named_array = unnest_matlab_struct_named_array(raw[key]) data = {k: named_array[k].squeeze() for k in named_array.dtype.names} return data Forcing Functions cryogrid_pytools.forcing.read_mat_ear5(filename) Read the ERA5.mat forcing file for CryoGrid and return a xarray Dataset. Parameters filename : str Path to the ERA5.mat file Returns xr.Dataset Dataset with the variables from the ERA5.mat file Source code in cryogrid_pytools/forcing.py def read_mat_ear5(filename: str) -> xr.Dataset: \"\"\" Read the ERA5.mat forcing file for CryoGrid and return a xarray Dataset. Parameters ---------- filename : str Path to the ERA5.mat file Returns ------- xr.Dataset Dataset with the variables from the ERA5.mat file \"\"\" import pathlib from .matlab_helpers import read_mat_struct_flat_as_dict filename = pathlib.Path(filename).expanduser().absolute().resolve() dat = read_mat_struct_flat_as_dict(filename) out = _era5_mat_dict_to_xarray(dat) out = out.assign_attrs( info=( \"Data read in from CryoGrid ERA5 forcing file. \" \"Data has been scaled to the original units with some modifications - units are given. \" \"Data has been transposed from [lon, lat, level, time] --> [time, level, lat, lon]. \" \"See the ERA5 documentation for more info about the units etc.\" ), source=filename, ) return out cryogrid_pytools.forcing.era5_to_matlab(ds, save_path=None) Convert a merged netCDF file from the Copernicus CDS to a dictionary that matches the expected format of the CryoGrid.POST_PROC.read_mat_ERA class (in MATLAB). Parameters ds : xr.Dataset Dataset from the ERA5 Copernicus CDS with variables required for the CryoGrid.POST_PROC.read_mat_ERA class single_levels = [u10, v10, sp, d2m, t2m, ssrd, strd, tisr, tp, Zs] pressure_levels = [t, z, q, u, v] Note that Zs in the single levels is a special case since it is only downloaded for a single date at the surface (doesn't change over time) save_path : str, optional Path to save the dictionary as a .mat file, by default None, meaning no file is saved and only the dictionary is returned Returns dict Dictionary with the variables mapped to names that are expected by CryoGrid.POST_PROC.read_mat_ERA Source code in cryogrid_pytools/forcing.py def era5_to_matlab(ds: xr.Dataset, save_path: str = None) -> dict: \"\"\" Convert a merged netCDF file from the Copernicus CDS to a dictionary that matches the expected format of the CryoGrid.POST_PROC.read_mat_ERA class (in MATLAB). Parameters ---------- ds : xr.Dataset Dataset from the ERA5 Copernicus CDS with variables required for the CryoGrid.POST_PROC.read_mat_ERA class single_levels = [u10, v10, sp, d2m, t2m, ssrd, strd, tisr, tp, Zs] pressure_levels = [t, z, q, u, v] Note that Zs in the single levels is a special case since it is only downloaded for a single date at the surface (doesn't change over time) save_path : str, optional Path to save the dictionary as a .mat file, by default None, meaning no file is saved and only the dictionary is returned Returns ------- dict Dictionary with the variables mapped to names that are expected by CryoGrid.POST_PROC.read_mat_ERA \"\"\" import numpy as np from .matlab_helpers import datetime2matlab # transpose to lon x lat x time (original is time x lat x lon) ds = ds.transpose(\"longitude\", \"latitude\", \"level\", \"time\") era = dict() era[\"dims\"] = \"lon x lat (x pressure_levels) x time\" # while lat and lon have to be [coord x 1] era[\"lat\"] = ds[\"latitude\"].values[:, None] era[\"lon\"] = ds[\"longitude\"].values[:, None] # pressure levels have to be [1 x coord] - only when pressure_levels present era[\"p\"] = ds[\"level\"].values[None] * 100 # time for some reason has to be [1 x coord] era[\"t\"] = datetime2matlab(ds.time)[None] # geopotential height at surface era[\"Zs\"] = ds.Zs.values / 9.81 # gravity m/s2 # single_level variables # wind and pressure (no transformations) era[\"u10\"] = ds[\"u10\"].values era[\"v10\"] = ds[\"v10\"].values era[\"ps\"] = ds[\"sp\"].values # temperature variables (degK -> degC) era[\"Td2\"] = ds[\"d2m\"].values - 273.15 era[\"T2\"] = ds[\"t2m\"].values - 273.15 # radiation variables (/sec -> /hour) era[\"SW\"] = ds[\"ssrd\"].values / 3600 era[\"LW\"] = ds[\"strd\"].values / 3600 era[\"S_TOA\"] = ds[\"tisr\"].values / 3600 # precipitation (m -> mm) era[\"P\"] = ds[\"tp\"].values * 1000 # pressure levels era[\"T\"] = ds[\"t\"].values - 273.15 # K to C era[\"Z\"] = ds[\"z\"].values / 9.81 # gravity m/s2 era[\"q\"] = ds[\"q\"].values era[\"u\"] = ds[\"u\"].values era[\"v\"] = ds[\"v\"].values # scaling factors era[\"wind_sf\"] = 1e-2 era[\"q_sf\"] = 1e-6 era[\"ps_sf\"] = 1e2 era[\"rad_sf\"] = 1e-1 era[\"T_sf\"] = 1e-2 era[\"P_sf\"] = 1e-2 # apply scaling factors (done in the original, so we do it here) # wind scaling era[\"u\"] = (era[\"u\"] / era[\"wind_sf\"]).astype(np.int16) era[\"v\"] = (era[\"v\"] / era[\"wind_sf\"]).astype(np.int16) era[\"u10\"] = (era[\"u10\"] / era[\"wind_sf\"]).astype(np.int16) era[\"v10\"] = (era[\"v10\"] / era[\"wind_sf\"]).astype(np.int16) # temperature scaling era[\"T\"] = (era[\"T\"] / era[\"T_sf\"]).astype(np.int16) era[\"Td2\"] = (era[\"Td2\"] / era[\"T_sf\"]).astype(np.int16) era[\"T2\"] = (era[\"T2\"] / era[\"T_sf\"]).astype(np.int16) # humidity scaling era[\"q\"] = (era[\"q\"] / era[\"q_sf\"]).astype(np.uint16) # pressure scaling era[\"ps\"] = (era[\"ps\"] / era[\"ps_sf\"]).astype(np.uint16) # radiation scaling era[\"SW\"] = (era[\"SW\"] / era[\"rad_sf\"]).astype(np.uint16) era[\"LW\"] = (era[\"LW\"] / era[\"rad_sf\"]).astype(np.uint16) era[\"S_TOA\"] = (era[\"S_TOA\"] / era[\"rad_sf\"]).astype(np.uint16) # precipitation scaling era[\"P\"] = (era[\"P\"] / era[\"P_sf\"]).astype(np.uint16) # no scaling for geoportential height era[\"Z\"] = era[\"Z\"].astype(np.int16) out = {\"era\": era} if save_path is not None and isinstance(save_path, str): from scipy.io import savemat savemat(save_path, out, appendmat=True, do_compression=True) return out Configuration Functions cryogrid_pytools.excel_config.CryoGridConfigExcel A class to read CryoGrid Excel configuration files and extract file paths and maybe in the future do some checks etc Source code in cryogrid_pytools/excel_config.py class CryoGridConfigExcel: \"\"\" A class to read CryoGrid Excel configuration files and extract file paths and maybe in the future do some checks etc \"\"\" def __init__(self, fname_xls: str, check_file_paths=True, check_strat_layers=True): \"\"\" Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach. Parameters ---------- fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible \"\"\" self.fname = pathlib.Path(fname_xls).resolve() self.root = self._get_root_path() self._df = self._load_xls(fname_xls) logger.success(f\"Loaded CryoGrid Excel configuration file: {self.fname}\") self.fname = Munch() self.fname.dem = self.get_dem_path() self.fname.coords = self.get_coord_path() self.fname.era5 = self.get_forcing_path() self.fname.datasets = self.get_dataset_paths() self.time = self.get_start_end_times() if check_file_paths: self.check_files_exist() if check_strat_layers: self.check_strat_layers() logger.info( f\"Start and end times: {self.time.time_start:%Y-%m-%d} - {self.time.time_end:%Y-%m-%d}\" ) def _get_root_path(self): \"\"\" Find and set the root path by locating the 'run_cryogrid.m' file. Returns ------- pathlib.Path The discovered root path or the current directory if not found. \"\"\" path = self.fname.parent while True: flist = path.glob(\"run_cryogrid.m\") if len(list(flist)) > 0: self.root = path logger.info(f\"Found root path: {path}\") return self.root elif str(path) == \"/\": logger.warning( \"Could not find root path. Set to current directory. You can change this manually with excel_config.root = pathlib.Path('/path/to/root')\" ) return pathlib.Path(\".\") else: path = path.parent def get_start_end_times(self): \"\"\" Retrieve the start and end times from the Excel configuration. Returns ------- pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. \"\"\" times = self.get_class(\"set_start_end_time\").T.filter(regex=\"time\") times = times.map( lambda x: pd.Timestamp(year=int(x[0]), month=int(x[1]), day=int(x[2])) ) start = times.start_time.min() end = times.end_time.max() times = pd.Series([start, end], index=[\"time_start\", \"time_end\"]) return times def get_coord_path(self): \"\"\" Get the path to the coordinates file from the Excel configuration. Returns ------- pathlib.Path The file path for coordinates. \"\"\" fname = self.get_class_filepath( \"COORDINATES_FROM_FILE\", fname_key=\"file_name\", index=1 ) return fname def get_dataset_paths(self): \"\"\" Retrieve paths for each dataset from the Excel configuration. Returns ------- munch.Munch A dictionary-like object mapping dataset variable names to file paths. \"\"\" paths = ( self.get_class_filepath(\"READ_DATASET\", fname_key=\"filename\") .to_frame(name=\"filepath\") .T ) datasets = self.get_class(\"READ_DATASET\") variable = datasets.T.variable_name paths.loc[\"variable\"] = variable paths = Munch(**paths.T.set_index(\"variable\").filepath.to_dict()) return paths def get_dem_path(self): \"\"\" Get the path for the DEM file from the Excel configuration. Returns ------- pathlib.Path The DEM file path. \"\"\" fname = self.get_class_filepath( \"DEM\", folder_key=\"folder\", fname_key=\"filename\", index=1 ) return fname def get_forcing_path(self, class_name=\"read_mat_ERA\"): \"\"\" Obtain the forcing file path from the Excel configuration. Parameters ---------- class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'. Returns ------- pathlib.Path The forcing file path. \"\"\" fname = self.get_class_filepath( class_name, folder_key=\"path\", fname_key=\"filename\", index=1 ) return fname def get_output_max_depth( self, output_class=\"OUT_regridded_FCI2\", depth_key=\"depth_below_ground\" ) -> int: \"\"\" Get the maximum depth of the output file from the Excel configuration. Parameters ---------- output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'. Returns ------- float The maximum depth value. \"\"\" df = self.get_class(output_class) depth = str(df.loc[depth_key].iloc[0]) depth = int(depth) return depth def check_forcing_fname_times(self): \"\"\" Check if the file name matches the forcing years specified in the Excel configuration. Raises ------ AssertionError If the forcing years in the file name do not match those in the configuration. \"\"\" import re fname = self.get_forcing_path() times = self.get_start_end_times().dt.year.astype(str).values.tolist() fname_years = re.findall(r\"[-_]([12][1089][0-9][0-9])\", fname.stem) assert times == fname_years, ( f\"File name years do not match the forcing years: forcing {times} != fname {fname_years}\" ) def _load_xls(self, fname_xls: str) -> pd.DataFrame: \"\"\" Load the Excel file into a DataFrame. Parameters ---------- fname_xls : str Path to the Excel file. Returns ------- pandas.DataFrame The loaded data with proper indexing and column names. \"\"\" import string alph = list(string.ascii_uppercase) alphabet_extra = alph + [a + b for a in alph for b in alph] df = pd.read_excel(fname_xls, header=None, dtype=str) df.columns = [c for c in alphabet_extra[: df.columns.size]] df.index = df.index + 1 return df def _get_unique_key(self, key: str, col_value=\"B\"): \"\"\" Retrieve a single unique value for a given key from the Excel data. Parameters ---------- key : str The key to look for in column 'A'. col_value : str, optional The column to retrieve the value from, by default 'B'. Returns ------- str or None The found value or None if no value exists. Raises ------ ValueError If multiple values are found for the given key. \"\"\" df = self._df idx = df.A == key value = df.loc[idx, col_value].values if len(value) == 0: return None elif len(value) > 1: raise ValueError(f\"Multiple values found for key: {key}\") else: return value[0] def get_classes(self): \"\"\" Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class(<class_name>). Returns ------- dict of int: str A dictionary mapping class names to row indices \"\"\" df = self._df class_idx = [] for i in range(len(df)): try: self._find_class_block(i) class_idx.append(i) except Exception: pass classes = df.loc[class_idx, \"A\"].to_dict() return classes def get_class_filepath( self, key, folder_key=\"folder\", fname_key=\"file\", index=None ): \"\"\" Construct a file path from folder and file entries in the Excel configuration. Parameters ---------- key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries. Returns ------- pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries. Raises ------ AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. \"\"\" df = self.get_class(key) keys = df.index.values folder_key = keys[[folder_key in k for k in keys]] fname_key = keys[[fname_key in k for k in keys]] assert len(folder_key) == 1, f\"Multiple folder keys found: {folder_key}\" assert len(fname_key) == 1, f\"Multiple fname keys found: {fname_key}\" names = df.loc[[folder_key[0], fname_key[0]]] names = names.apply(lambda ser: self.root / ser.iloc[0] / ser.iloc[1]) if index is None: return names elif isinstance(index, int): return names.loc[f\"{key}_{index}\"] else: raise TypeError(f\"index must be None or int, not {type(index)}\") def get_class(self, class_name: str) -> pd.DataFrame: \"\"\" Return DataFrame blocks representing the specified class from the Excel data. Parameters ---------- class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET'). Returns ------- pandas.DataFrame The concatenated DataFrame of class blocks. \"\"\" df = self._df i0s = df.A == class_name i0s = i0s[i0s].index.values blocks = [self._find_class_block(i0) for i0 in i0s] try: df = pd.concat(blocks, axis=1) except Exception: # only intended for debugging df = blocks logger.warning(f\"Could not concatenate blocks for class: {class_name}\") return df def _find_class_block(self, class_idx0: int): \"\"\" Identify and extract the block of rows corresponding to a class definition. Parameters ---------- class_idx0 : int Starting row index for the class in the Excel data. Returns ------- pandas.DataFrame The processed block as a DataFrame. Raises ------ AssertionError If the class structure is missing required indicators. \"\"\" df = self._df class_name = df.A.loc[class_idx0] msg = f\"Given class_idx0 ({class_name}) is not a class. Must have 'index' adjacent or on cell up and right.\" is_index = df.B.loc[class_idx0 - 1 : class_idx0].str.contains(\"index\") assert is_index.any(), msg index_idx = is_index.idxmax() class_idx0 = index_idx class_idx1 = df.A.loc[class_idx0:] == \"CLASS_END\" # get first True occurrence class_idx1 = class_idx1.idxmax() class_block = df.loc[class_idx0:class_idx1] class_block = self._process_class_block(class_block) return class_block def _process_class_block(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Process a raw class block by removing comments, handling special structures, and shaping data. Parameters ---------- df : pandas.DataFrame A DataFrame slice representing the raw class block. Returns ------- pandas.DataFrame The cleaned and structured DataFrame of class data. Raises ------ AssertionError When matrix structures in the block do not match expected format. \"\"\" \"\"\"hacky way to process the class block\"\"\" # drop CLASS_END row df = df[df.A != \"CLASS_END\"] # if any cell starts with '>', it is a comment df = df.map(lambda x: x if not str(x).startswith(\">\") else np.nan) # drop rows and columns that are all NaN df = df.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\") df = df.astype(str) # H_LIST and V_MATRIX are special cases contains_matrix = df.map(lambda x: \"MATRIX\" in x).values contains_vmatrix = df.map(lambda x: \"V_MATRIX\" in x).values contains_end = df.map(lambda x: \"END\" in x).values ends = np.where(contains_end) if contains_matrix.any(): r0, c0 = [a[0] for a in np.where(contains_matrix)] assert c0 == 1, \"Matrix must be in second column\" assert len(ends) == 2, \"Only two ENDs are allowed\" assert r0 == ends[0][0] assert c0 == ends[1][1] r1 = ends[0][1] c1 = ends[1][0] arr = df.iloc[r0:r1, c0:c1].values if contains_vmatrix.any(): # first column of V_MATRIX is the index but is not in the config file # so we create it. It is one shorter than the num of rows because of header arr[1:, 0] = np.arange(r1 - r0 - 1) matrix = pd.DataFrame(arr[1:, 1:], index=arr[1:, 0], columns=arr[0, 1:]) matrix.index.name = matrix.columns.name = df.iloc[r0, 0] df = df.drop(index=df.index[r0 : r1 + 1]) df.loc[r0, \"A\"] = matrix.index.name df.loc[r0, \"B\"] = (matrix.to_dict(),) for i, row in df.iterrows(): # H_LIST first if row.str.contains(\"H_LIST\").any(): r0 = 2 r1 = row.str.contains(\"END\").argmax() df.loc[i, \"B\"] = row.iloc[r0:r1].values.tolist() class_category = df.A.iloc[0] class_type = df.A.iloc[1] class_index = df.B.iloc[1] col_name = f\"{class_type}_{class_index}\" df = df.iloc[2:, :2].rename(columns=dict(B=col_name)).set_index(\"A\") df.index.name = class_category return df def check_strat_layers(self): \"\"\" Run checks to ensure stratigraphy layers have physically plausible parameter values. \"\"\" strat_layers = self.get_class(\"STRAT_layers\") logger.info(\"Checking stratigraphy layers...\") for layer in strat_layers: try: check_strat_layer_values(strat_layers[layer].iloc[0]) logger.success(f\"[{layer}] parameters passed checks\") except ValueError as error: logger.warning(f\"[{layer}] {error}\") def check_files_exist(self): \"\"\" Check if all the files in the configuration exist. \"\"\" flist = set( [self.get_forcing_path(), self.get_dem_path(), self.get_coord_path()] + list(self.get_dataset_paths().values()) ) logger.info(\"Checking file locations...\") for f in flist: if not f.exists(): logger.warning(f\"Cannot find file: {f}\") else: logger.success(f\"Located file: {f}\") __init__(fname_xls, check_file_paths=True, check_strat_layers=True) Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach. Parameters fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible Source code in cryogrid_pytools/excel_config.py def __init__(self, fname_xls: str, check_file_paths=True, check_strat_layers=True): \"\"\" Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach. Parameters ---------- fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible \"\"\" self.fname = pathlib.Path(fname_xls).resolve() self.root = self._get_root_path() self._df = self._load_xls(fname_xls) logger.success(f\"Loaded CryoGrid Excel configuration file: {self.fname}\") self.fname = Munch() self.fname.dem = self.get_dem_path() self.fname.coords = self.get_coord_path() self.fname.era5 = self.get_forcing_path() self.fname.datasets = self.get_dataset_paths() self.time = self.get_start_end_times() if check_file_paths: self.check_files_exist() if check_strat_layers: self.check_strat_layers() logger.info( f\"Start and end times: {self.time.time_start:%Y-%m-%d} - {self.time.time_end:%Y-%m-%d}\" ) check_files_exist() Check if all the files in the configuration exist. Source code in cryogrid_pytools/excel_config.py def check_files_exist(self): \"\"\" Check if all the files in the configuration exist. \"\"\" flist = set( [self.get_forcing_path(), self.get_dem_path(), self.get_coord_path()] + list(self.get_dataset_paths().values()) ) logger.info(\"Checking file locations...\") for f in flist: if not f.exists(): logger.warning(f\"Cannot find file: {f}\") else: logger.success(f\"Located file: {f}\") check_forcing_fname_times() Check if the file name matches the forcing years specified in the Excel configuration. Raises AssertionError If the forcing years in the file name do not match those in the configuration. Source code in cryogrid_pytools/excel_config.py def check_forcing_fname_times(self): \"\"\" Check if the file name matches the forcing years specified in the Excel configuration. Raises ------ AssertionError If the forcing years in the file name do not match those in the configuration. \"\"\" import re fname = self.get_forcing_path() times = self.get_start_end_times().dt.year.astype(str).values.tolist() fname_years = re.findall(r\"[-_]([12][1089][0-9][0-9])\", fname.stem) assert times == fname_years, ( f\"File name years do not match the forcing years: forcing {times} != fname {fname_years}\" ) check_strat_layers() Run checks to ensure stratigraphy layers have physically plausible parameter values. Source code in cryogrid_pytools/excel_config.py def check_strat_layers(self): \"\"\" Run checks to ensure stratigraphy layers have physically plausible parameter values. \"\"\" strat_layers = self.get_class(\"STRAT_layers\") logger.info(\"Checking stratigraphy layers...\") for layer in strat_layers: try: check_strat_layer_values(strat_layers[layer].iloc[0]) logger.success(f\"[{layer}] parameters passed checks\") except ValueError as error: logger.warning(f\"[{layer}] {error}\") get_class(class_name) Return DataFrame blocks representing the specified class from the Excel data. Parameters class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET'). Returns pandas.DataFrame The concatenated DataFrame of class blocks. Source code in cryogrid_pytools/excel_config.py def get_class(self, class_name: str) -> pd.DataFrame: \"\"\" Return DataFrame blocks representing the specified class from the Excel data. Parameters ---------- class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET'). Returns ------- pandas.DataFrame The concatenated DataFrame of class blocks. \"\"\" df = self._df i0s = df.A == class_name i0s = i0s[i0s].index.values blocks = [self._find_class_block(i0) for i0 in i0s] try: df = pd.concat(blocks, axis=1) except Exception: # only intended for debugging df = blocks logger.warning(f\"Could not concatenate blocks for class: {class_name}\") return df get_class_filepath(key, folder_key='folder', fname_key='file', index=None) Construct a file path from folder and file entries in the Excel configuration. Parameters key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries. Returns pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries. Raises AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. Source code in cryogrid_pytools/excel_config.py def get_class_filepath( self, key, folder_key=\"folder\", fname_key=\"file\", index=None ): \"\"\" Construct a file path from folder and file entries in the Excel configuration. Parameters ---------- key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries. Returns ------- pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries. Raises ------ AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. \"\"\" df = self.get_class(key) keys = df.index.values folder_key = keys[[folder_key in k for k in keys]] fname_key = keys[[fname_key in k for k in keys]] assert len(folder_key) == 1, f\"Multiple folder keys found: {folder_key}\" assert len(fname_key) == 1, f\"Multiple fname keys found: {fname_key}\" names = df.loc[[folder_key[0], fname_key[0]]] names = names.apply(lambda ser: self.root / ser.iloc[0] / ser.iloc[1]) if index is None: return names elif isinstance(index, int): return names.loc[f\"{key}_{index}\"] else: raise TypeError(f\"index must be None or int, not {type(index)}\") get_classes() Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class( ). Returns dict of int: str A dictionary mapping class names to row indices Source code in cryogrid_pytools/excel_config.py def get_classes(self): \"\"\" Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class(<class_name>). Returns ------- dict of int: str A dictionary mapping class names to row indices \"\"\" df = self._df class_idx = [] for i in range(len(df)): try: self._find_class_block(i) class_idx.append(i) except Exception: pass classes = df.loc[class_idx, \"A\"].to_dict() return classes get_coord_path() Get the path to the coordinates file from the Excel configuration. Returns pathlib.Path The file path for coordinates. Source code in cryogrid_pytools/excel_config.py def get_coord_path(self): \"\"\" Get the path to the coordinates file from the Excel configuration. Returns ------- pathlib.Path The file path for coordinates. \"\"\" fname = self.get_class_filepath( \"COORDINATES_FROM_FILE\", fname_key=\"file_name\", index=1 ) return fname get_dataset_paths() Retrieve paths for each dataset from the Excel configuration. Returns munch.Munch A dictionary-like object mapping dataset variable names to file paths. Source code in cryogrid_pytools/excel_config.py def get_dataset_paths(self): \"\"\" Retrieve paths for each dataset from the Excel configuration. Returns ------- munch.Munch A dictionary-like object mapping dataset variable names to file paths. \"\"\" paths = ( self.get_class_filepath(\"READ_DATASET\", fname_key=\"filename\") .to_frame(name=\"filepath\") .T ) datasets = self.get_class(\"READ_DATASET\") variable = datasets.T.variable_name paths.loc[\"variable\"] = variable paths = Munch(**paths.T.set_index(\"variable\").filepath.to_dict()) return paths get_dem_path() Get the path for the DEM file from the Excel configuration. Returns pathlib.Path The DEM file path. Source code in cryogrid_pytools/excel_config.py def get_dem_path(self): \"\"\" Get the path for the DEM file from the Excel configuration. Returns ------- pathlib.Path The DEM file path. \"\"\" fname = self.get_class_filepath( \"DEM\", folder_key=\"folder\", fname_key=\"filename\", index=1 ) return fname get_forcing_path(class_name='read_mat_ERA') Obtain the forcing file path from the Excel configuration. Parameters class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'. Returns pathlib.Path The forcing file path. Source code in cryogrid_pytools/excel_config.py def get_forcing_path(self, class_name=\"read_mat_ERA\"): \"\"\" Obtain the forcing file path from the Excel configuration. Parameters ---------- class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'. Returns ------- pathlib.Path The forcing file path. \"\"\" fname = self.get_class_filepath( class_name, folder_key=\"path\", fname_key=\"filename\", index=1 ) return fname get_output_max_depth(output_class='OUT_regridded_FCI2', depth_key='depth_below_ground') Get the maximum depth of the output file from the Excel configuration. Parameters output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'. Returns float The maximum depth value. Source code in cryogrid_pytools/excel_config.py def get_output_max_depth( self, output_class=\"OUT_regridded_FCI2\", depth_key=\"depth_below_ground\" ) -> int: \"\"\" Get the maximum depth of the output file from the Excel configuration. Parameters ---------- output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'. Returns ------- float The maximum depth value. \"\"\" df = self.get_class(output_class) depth = str(df.loc[depth_key].iloc[0]) depth = int(depth) return depth get_start_end_times() Retrieve the start and end times from the Excel configuration. Returns pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. Source code in cryogrid_pytools/excel_config.py def get_start_end_times(self): \"\"\" Retrieve the start and end times from the Excel configuration. Returns ------- pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. \"\"\" times = self.get_class(\"set_start_end_time\").T.filter(regex=\"time\") times = times.map( lambda x: pd.Timestamp(year=int(x[0]), month=int(x[1]), day=int(x[2])) ) start = times.start_time.min() end = times.end_time.max() times = pd.Series([start, end], index=[\"time_start\", \"time_end\"]) return times cryogrid_pytools.excel_config.check_strat_layer_values(tuple_containing_dict) Validate that stratigraphy layer parameters are physically plausible. Parameters tuple_containing_dict : tuple A tuple containing a dictionary whose keys represent layer parameters. Raises ValueError If any parameter check fails for the stratigraphy layers. Notes Definitions porosity = 1 - mineral - organic airspace = porosity - waterIce volume = mineral + organic + waterIce Checks field_capacity < porosity : field capacity is a subset of the porosity airspace >= 0 : cannot have negative airspace volume <= 1 : the sum of mineral, organic, and waterIce cannot exceed 1 waterIce <= porosity : waterIce cannot exceed porosity Source code in cryogrid_pytools/excel_config.py def check_strat_layer_values(tuple_containing_dict): \"\"\" Validate that stratigraphy layer parameters are physically plausible. Parameters ---------- tuple_containing_dict : tuple A tuple containing a dictionary whose keys represent layer parameters. Raises ------ ValueError If any parameter check fails for the stratigraphy layers. Notes ----- #### Definitions - `porosity = 1 - mineral - organic` - `airspace = porosity - waterIce` - `volume = mineral + organic + waterIce` #### Checks - `field_capacity < porosity` : field capacity is a subset of the porosity - `airspace >= 0` : cannot have negative airspace - `volume <= 1` : the sum of mineral, organic, and waterIce cannot exceed 1 - `waterIce <= porosity` : waterIce cannot exceed porosity \"\"\" dictionary = tuple_containing_dict[0] df = pd.DataFrame(dictionary).astype(float).round(3) df[\"porosity\"] = (1 - df.mineral - df.organic).round(3) df[\"airspace\"] = (df.porosity - df.waterIce).round(3) df[\"volume\"] = (df.mineral + df.organic + df.waterIce).round(3) checks = pd.DataFrame() checks[\"field_capacity_lt_porosity\"] = df.field_capacity <= df.porosity checks[\"airspace_ge_0\"] = df.airspace >= 0 checks[\"volume_le_1\"] = df.volume <= 1 checks[\"waterice_le_porosity\"] = df.waterIce <= df.porosity checks.index.name = \"layer\" if not checks.values.all(): raise ValueError( \"parameters are not physically plausible. \" \"below are the violations: \\n\" + str(checks.T) ) Data Module Functions cryogrid_pytools.data.get_dem_copernicus30(bbox_WSEN, res_m=30, epsg=32643, smoothing_iters=2, smoothing_size=3) Download DEM data from the STAC catalog (default is COP DEM Global 30m). Parameters bbox_WSEN : list The bounding box of the area of interest in WSEN format. res_m : int The resolution of the DEM data in meters. epsg : int, optional The EPSG code of the projection of the DEM data. Default is EPSG:32643 (UTM 43N) for the Pamir region. smoothing_iters : int, optional The number of iterations to apply the smoothing filter. Default is 2. Set to 0 to disable smoothing. smoothing_size : int, optional The size of the kernel (num pixels) for the smoothing filter. Default is 3. Returns xarray.DataArray The DEM data as an xarray DataArray with attributes. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_dem_copernicus30( bbox_WSEN: list, res_m: int = 30, epsg=32643, smoothing_iters=2, smoothing_size=3 ) -> _xr.DataArray: \"\"\" Download DEM data from the STAC catalog (default is COP DEM Global 30m). Parameters ---------- bbox_WSEN : list The bounding box of the area of interest in WSEN format. res_m : int The resolution of the DEM data in meters. epsg : int, optional The EPSG code of the projection of the DEM data. Default is EPSG:32643 (UTM 43N) for the Pamir region. smoothing_iters : int, optional The number of iterations to apply the smoothing filter. Default is 2. Set to 0 to disable smoothing. smoothing_size : int, optional The size of the kernel (num pixels) for the smoothing filter. Default is 3. Returns ------- xarray.DataArray The DEM data as an xarray DataArray with attributes. \"\"\" from .utils import drop_coords_without_dim check_epsg(epsg) assert res_m >= 30, ( \"The resolution must be greater than 30m for the COP DEM Global 30m dataset.\" ) res = res_m / 111111 if epsg == 4326 else res_m _logger.info(\"Fetching COP DEM Global 30m data from Planetary Computer\") items = search_stac_items_planetary_computer(\"cop-dem-glo-30\", bbox_WSEN) da_dem = _stackstac.stack( items=items, bounds_latlon=bbox_WSEN, resolution=res, epsg=epsg ) da_dem = ( da_dem.mean(\"time\") .squeeze() .pipe(drop_coords_without_dim) .pipe(smooth_data, n_iters=smoothing_iters, kernel_size=smoothing_size) .rio.write_crs(f\"EPSG:{epsg}\") .assign_attrs( source=items[0].links[0].href, # collection URL bbox_request=bbox_WSEN, ) ) return da_dem cryogrid_pytools.data.get_esa_land_cover(bbox_WSEN, res_m=30, epsg=32643) Get the ESA World Cover dataset on the target grid and resolution. Parameters bbox_WSEN : tuple Bounding box in the format (West, South, East, North). res_m : int, optional Resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns xr.DataArray A DataArray with the land cover data on the target grid. Contains attributes 'class_values', 'class_descriptions', 'class_colors' for plotting. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_esa_land_cover(bbox_WSEN: tuple, res_m: int = 30, epsg=32643) -> _xr.DataArray: \"\"\" Get the ESA World Cover dataset on the target grid and resolution. Parameters ---------- bbox_WSEN : tuple Bounding box in the format (West, South, East, North). res_m : int, optional Resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns ------- xr.DataArray A DataArray with the land cover data on the target grid. Contains attributes 'class_values', 'class_descriptions', 'class_colors' for plotting. \"\"\" def get_land_cover_classes(item): \"\"\" Get the land cover class names, and colors from the ESA World Cover dataset Args: item (pystac.Item): The STAC item containing the land cover data. Returns: dict: A dictionary with class values, descriptions, and colors. \"\"\" import pandas as pd classes = item.assets[\"map\"].extra_fields[\"classification:classes\"] df = ( pd.DataFrame(classes) .set_index(\"value\") .rename( columns=lambda s: s.replace(\"-\", \"_\") ) # bug fix for version 2.7.8 (stacstack back compatibility) ) df[\"color_hint\"] = \"#\" + df[\"color_hint\"] out = dict( class_values=df.index.values, class_descriptions=df[\"description\"].values, class_colors=df[\"color_hint\"].values, ) return out # make sure epsg is supported check_epsg(epsg) # get the units in the projection res = get_res_in_proj_units(res_m, epsg, min_res=10) _logger.info(\"Fetching ESA World Cover (v2.0) data from Planetary Computer\") items = search_stac_items_planetary_computer( collection=\"esa-worldcover\", bbox=bbox_WSEN, query={\"esa_worldcover:product_version\": {\"eq\": \"2.0.0\"}}, ) stac_props = dict( items=items, assets=[\"map\"], epsg=epsg, bounds_latlon=bbox_WSEN, resolution=res ) da = ( _stackstac.stack(**stac_props) .max([\"band\", \"time\"], keep_attrs=True) # removing the single band dimension .rename(\"land_cover\") .assign_attrs(**get_land_cover_classes(items[0])) ) return da cryogrid_pytools.data.get_snow_melt_doy(bbox_WSEN, years=range(2018, 2025), res_m=30, epsg=32643) Calculate the snow melt day of year (DOY) from Sentinel-2 SCL data for a given bounding box and years. Parameters bbox_WSEN : tuple Bounding box coordinates in the format (West, South, East, North). years : range, optional Range of years to consider. Defaults to range(2018, 2025). res_m : int, optional Spatial resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns _xr.DataArray DataArray containing the snow melt DOY for each year. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_snow_melt_doy( bbox_WSEN: tuple, years=range(2018, 2025), res_m: int = 30, epsg=32643 ) -> _xr.DataArray: \"\"\" Calculate the snow melt day of year (DOY) from Sentinel-2 SCL data for a given bounding box and years. Parameters ---------- bbox_WSEN : tuple Bounding box coordinates in the format (West, South, East, North). years : range, optional Range of years to consider. Defaults to range(2018, 2025). res_m : int, optional Spatial resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns ------- _xr.DataArray DataArray containing the snow melt DOY for each year. \"\"\" da = get_sentinel2_data( bbox_WSEN, years=years, res_m=res_m, epsg=epsg, max_cloud_cover=10 ) _logger.info(\"Calculating snow melt day of year (DOY) from Sentinel-2 SCL data\") doy = da.groupby(\"time.year\").apply(calc_sentinel2_snow_melt_doy) return doy cryogrid_pytools.data.get_randolph_glacier_inventory(target_dem=None, dest_dir=None) Fetches the Randolph Glacier Inventory (RGI) data and returns it as a GeoDataFrame or raster dataset. Parameters target_dem : optional A digital elevation model (DEM) object. If provided, the function will return the RGI data clipped to the bounding box of the DEM and reprojected to the DEM's CRS. dest_dir : str, optional The directory where the downloaded RGI data will be stored. If None, the data will be stored in the pooch cache directory (~/.cache/pooch/). Returns GeoDataFrame or raster dataset If target_dem is None, returns a GeoDataFrame containing the RGI data. If target_dem is provided, returns a raster dataset clipped and reprojected to the DEM. Source code in cryogrid_pytools/data.py @_cached def get_randolph_glacier_inventory(target_dem=None, dest_dir=None): \"\"\" Fetches the Randolph Glacier Inventory (RGI) data and returns it as a GeoDataFrame or raster dataset. Parameters ---------- target_dem : optional A digital elevation model (DEM) object. If provided, the function will return the RGI data clipped to the bounding box of the DEM and reprojected to the DEM's CRS. dest_dir : str, optional The directory where the downloaded RGI data will be stored. If None, the data will be stored in the pooch cache directory (~/.cache/pooch/). Returns ------- GeoDataFrame or raster dataset If target_dem is None, returns a GeoDataFrame containing the RGI data. If target_dem is provided, returns a raster dataset clipped and reprojected to the DEM. \"\"\" url = \"https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0770_rgi_v7/regional_files/RGI2000-v7.0-G/RGI2000-v7.0-G-13_central_asia.zip\" downloader = _pooch.HTTPDownloader( progressbar=True, headers=get_earthaccess_session().headers ) flist = download_url(url, path=dest_dir, downloader=downloader) fname_shp = [f for f in flist if f.endswith(\".shp\")][0] _logger.log( \"INFO\", \"RGI: Fetching Randolph Glacier Inventory - see https://www.glims.org/rgi_user_guide/welcome.html\", ) _logger.log(\"DEBUG\", f\"RGI: URL = {url}\") _logger.log(\"DEBUG\", f\"RGI: FILE = {fname_shp}\") if target_dem is None: # reads the whole file df = _gpd.read_file(fname_shp) else: # gets the bounding box and then reads the file bbox = target_dem.rv.get_bbox_latlon() df = _gpd.read_file(fname_shp, bbox=bbox).to_crs(target_dem.rio.crs) df = df.dissolve() ds = df.rv.to_raster(target_dem) return ds return df Utility Functions Additional utility functions are available in the package. See the source code documentation for more details.","title":"API Reference"},{"location":"api-reference/#api-reference","text":"This page contains the detailed API reference for CryoGrid-pyTools.","title":"API Reference"},{"location":"api-reference/#main-functions","text":"","title":"Main Functions"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_file","text":"Read a CryoGrid OUT_regridded[_FCI2] file and return it as an xarray dataset.","title":"read_OUT_regridded_file"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_file--parameters","text":"fname : str Path to the .mat file deepest_point : float, optional Represents the deepest depth of the profile relative to the surfface. If not provided, then elevation is returned. Negative values represent depths below the surface.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_file--returns","text":"ds : xarray.Dataset Dataset with dimensions 'time' and 'level'. The elevation coordinate represents the elevation above sea level based on the DEM. If deepest_point is provided, then an additional coordinate, depth , represents depth above surface.","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_file--notes","text":"For plotting, use `ds['variable'].plot(y='depth'/'elevation'). Source code in cryogrid_pytools/outputs.py def read_OUT_regridded_file(fname: str, deepest_point=None) -> xr.Dataset: \"\"\" Read a CryoGrid OUT_regridded[_FCI2] file and return it as an xarray dataset. Parameters ---------- fname : str Path to the .mat file deepest_point : float, optional Represents the deepest depth of the profile relative to the surfface. If not provided, then elevation is returned. Negative values represent depths below the surface. Returns ------- ds : xarray.Dataset Dataset with dimensions 'time' and 'level'. The `elevation` coordinate represents the elevation above sea level based on the DEM. If deepest_point is provided, then an additional coordinate, `depth`, represents depth above surface. Notes ----- For plotting, use `ds['variable'].plot(y='depth'/'elevation'). \"\"\" from cryogrid_pytools.matlab_helpers import ( matlab2datetime, read_mat_struct_flat_as_dict, ) dat = read_mat_struct_flat_as_dict(fname) for key in dat: dat[key] = dat[key].squeeze() ds = xr.Dataset() ds.attrs[\"filename\"] = fname times = matlab2datetime(dat.pop(\"timestamp\")) elev = dat.pop(\"depths\") for key in dat: ds[key] = xr.DataArray( data=dat[key].astype(\"float32\"), dims=[\"level\", \"time\"], coords={\"time\": times}, ) ds = ds.chunk(dict(time=-1)) ds[\"elevation\"] = xr.DataArray( data=elev, dims=[\"level\"], attrs={\"units\": \"m\", \"long_name\": \"Elevation above sea level\"}, ) ds = ds.set_coords(\"elevation\") if deepest_point is not None: dz = deepest_point - ds[\"elevation\"].min() ds[\"depth\"] = (ds[\"elevation\"] + dz).assign_attrs( units=\"m\", long_name=\"Depth relative to surface\" ) ds = ds.set_coords(\"depth\") ds = ds.chunk(dict(time=-1)) return ds","title":"Notes"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_clusters","text":"Reads multiple files that are put out by the OUT_regridded class (and _FCI2)","title":"read_OUT_regridded_clusters"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_clusters--parameters","text":"fname_glob: str Path of the files that you want to read in. Use same notation as for glob(). Note that it expects name to follow the format some_project_name_GRIDCELL_ID_date.mat where GRIDCELL_ID will be extracted to assign the gridcell dimension. These GRIDCELL_IDs correspond with the index of the data in the flattened array. deepest_point: float When setting the configuration for when the data should be saved, the maximum depth is set. Give this number as a negative number here. joblib_kwargs: dict Uses the joblib library to do parallel reading of the files. Defaults are: n_jobs=-1, backend='threading', verbose=1","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.read_OUT_regridded_clusters--returns","text":"xr.Dataset An array with dimensions gridcell, depth, time. Variables depend on how the class was configured, but elevation will also be a variable. Source code in cryogrid_pytools/outputs.py def read_OUT_regridded_clusters( fname_glob: str, deepest_point: float, gridcell_func=lambda fname: fname.split(\"_\")[-2], **joblib_kwargs, ) -> xr.Dataset: \"\"\" Reads multiple files that are put out by the OUT_regridded class (and _FCI2) Parameters ---------- fname_glob: str Path of the files that you want to read in. Use same notation as for glob(). Note that it expects name to follow the format `some_project_name_GRIDCELL_ID_date.mat` where GRIDCELL_ID will be extracted to assign the gridcell dimension. These GRIDCELL_IDs correspond with the index of the data in the flattened array. deepest_point: float When setting the configuration for when the data should be saved, the maximum depth is set. Give this number as a negative number here. joblib_kwargs: dict Uses the joblib library to do parallel reading of the files. Defaults are: n_jobs=-1, backend='threading', verbose=1 Returns ------- xr.Dataset An array with dimensions gridcell, depth, time. Variables depend on how the class was configured, but elevation will also be a variable. \"\"\" import inspect from .utils import regex_glob # get the file list flist = regex_glob(fname_glob) # extract the gridcell from the file name gridcell = [gridcell_func(f) for f in flist] digits = [g.isdigit() for g in gridcell] if len(flist) == 0: raise FileNotFoundError(f\"No files found with {fname_glob}\") elif len(gridcell) != len(flist): raise ValueError(f\"Could not extract gridcell from file names for {fname_glob}\") elif not all(digits): not_digit = np.unique([f for f, d in zip(flist, digits) if not d]) bad_func = \"\".join(inspect.getsource(gridcell_func).split(\"lambda\")[1:]).strip() raise ValueError( f\"Check your fname_glob ({fname_glob}) \\n or gridcell_func ({bad_func}). \\nGridcell ID \" f\"not a number for the following files:\\n{not_digit}\" ) else: gridcell = [int(g) for g in gridcell] list_of_ds = _read_OUT_regridded_parallel(flist, deepest_point, **joblib_kwargs) # assign the gridcell dimension so that we can combine the data by coordinates and time list_of_ds = [ds.expand_dims(gridcell=[c]) for ds, c in zip(list_of_ds, gridcell)] ds = xr.combine_by_coords(list_of_ds, combine_attrs=\"drop_conflicts\") assert isinstance(ds, xr.Dataset), \"Something went wrong with the parallel reading.\" # transpose data so that plotting is quick and easy ds = ds.transpose(\"gridcell\", \"level\", \"time\", ...) return ds","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.matlab_helpers.read_mat_struct_flat_as_dict","text":"Read a MATLAB struct from a .mat file and return it as a dictionary. Assumes that the struct is flat, i.e. it does not contain any nested structs.","title":"read_mat_struct_flat_as_dict"},{"location":"api-reference/#cryogrid_pytools.matlab_helpers.read_mat_struct_flat_as_dict--parameters","text":"fname : str Path to the .mat file key : str, optional The name of the matlab key in the .mat file. If None is passed [default], then the first key that does not start with an underscore is used. If a string is passed, then the corresponding key is used.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.matlab_helpers.read_mat_struct_flat_as_dict--returns","text":"data : dict Dictionary with the struct fields as keys and the corresponding data as values. Source code in cryogrid_pytools/matlab_helpers.py def read_mat_struct_flat_as_dict(fname: str, key=None) -> dict: \"\"\" Read a MATLAB struct from a .mat file and return it as a dictionary. Assumes that the struct is flat, i.e. it does not contain any nested structs. Parameters ---------- fname : str Path to the .mat file key : str, optional The name of the matlab key in the .mat file. If None is passed [default], then the first key that does not start with an underscore is used. If a string is passed, then the corresponding key is used. Returns ------- data : dict Dictionary with the struct fields as keys and the corresponding data as values. \"\"\" from scipy.io import loadmat raw = loadmat(fname) keys = [k for k in raw.keys() if not k.startswith(\"_\")] if key is None: logger.log( 5, f\"No key specified. Using first key that does not start with an underscore: {keys[0]}\", ) key = keys[0] elif key not in keys: raise ValueError( f\"Key '{key}' not found in .mat file. Available keys are: {keys}\" ) named_array = unnest_matlab_struct_named_array(raw[key]) data = {k: named_array[k].squeeze() for k in named_array.dtype.names} return data","title":"Returns"},{"location":"api-reference/#forcing-functions","text":"","title":"Forcing Functions"},{"location":"api-reference/#cryogrid_pytools.forcing.read_mat_ear5","text":"Read the ERA5.mat forcing file for CryoGrid and return a xarray Dataset.","title":"read_mat_ear5"},{"location":"api-reference/#cryogrid_pytools.forcing.read_mat_ear5--parameters","text":"filename : str Path to the ERA5.mat file","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.forcing.read_mat_ear5--returns","text":"xr.Dataset Dataset with the variables from the ERA5.mat file Source code in cryogrid_pytools/forcing.py def read_mat_ear5(filename: str) -> xr.Dataset: \"\"\" Read the ERA5.mat forcing file for CryoGrid and return a xarray Dataset. Parameters ---------- filename : str Path to the ERA5.mat file Returns ------- xr.Dataset Dataset with the variables from the ERA5.mat file \"\"\" import pathlib from .matlab_helpers import read_mat_struct_flat_as_dict filename = pathlib.Path(filename).expanduser().absolute().resolve() dat = read_mat_struct_flat_as_dict(filename) out = _era5_mat_dict_to_xarray(dat) out = out.assign_attrs( info=( \"Data read in from CryoGrid ERA5 forcing file. \" \"Data has been scaled to the original units with some modifications - units are given. \" \"Data has been transposed from [lon, lat, level, time] --> [time, level, lat, lon]. \" \"See the ERA5 documentation for more info about the units etc.\" ), source=filename, ) return out","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.forcing.era5_to_matlab","text":"Convert a merged netCDF file from the Copernicus CDS to a dictionary that matches the expected format of the CryoGrid.POST_PROC.read_mat_ERA class (in MATLAB).","title":"era5_to_matlab"},{"location":"api-reference/#cryogrid_pytools.forcing.era5_to_matlab--parameters","text":"ds : xr.Dataset Dataset from the ERA5 Copernicus CDS with variables required for the CryoGrid.POST_PROC.read_mat_ERA class single_levels = [u10, v10, sp, d2m, t2m, ssrd, strd, tisr, tp, Zs] pressure_levels = [t, z, q, u, v] Note that Zs in the single levels is a special case since it is only downloaded for a single date at the surface (doesn't change over time) save_path : str, optional Path to save the dictionary as a .mat file, by default None, meaning no file is saved and only the dictionary is returned","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.forcing.era5_to_matlab--returns","text":"dict Dictionary with the variables mapped to names that are expected by CryoGrid.POST_PROC.read_mat_ERA Source code in cryogrid_pytools/forcing.py def era5_to_matlab(ds: xr.Dataset, save_path: str = None) -> dict: \"\"\" Convert a merged netCDF file from the Copernicus CDS to a dictionary that matches the expected format of the CryoGrid.POST_PROC.read_mat_ERA class (in MATLAB). Parameters ---------- ds : xr.Dataset Dataset from the ERA5 Copernicus CDS with variables required for the CryoGrid.POST_PROC.read_mat_ERA class single_levels = [u10, v10, sp, d2m, t2m, ssrd, strd, tisr, tp, Zs] pressure_levels = [t, z, q, u, v] Note that Zs in the single levels is a special case since it is only downloaded for a single date at the surface (doesn't change over time) save_path : str, optional Path to save the dictionary as a .mat file, by default None, meaning no file is saved and only the dictionary is returned Returns ------- dict Dictionary with the variables mapped to names that are expected by CryoGrid.POST_PROC.read_mat_ERA \"\"\" import numpy as np from .matlab_helpers import datetime2matlab # transpose to lon x lat x time (original is time x lat x lon) ds = ds.transpose(\"longitude\", \"latitude\", \"level\", \"time\") era = dict() era[\"dims\"] = \"lon x lat (x pressure_levels) x time\" # while lat and lon have to be [coord x 1] era[\"lat\"] = ds[\"latitude\"].values[:, None] era[\"lon\"] = ds[\"longitude\"].values[:, None] # pressure levels have to be [1 x coord] - only when pressure_levels present era[\"p\"] = ds[\"level\"].values[None] * 100 # time for some reason has to be [1 x coord] era[\"t\"] = datetime2matlab(ds.time)[None] # geopotential height at surface era[\"Zs\"] = ds.Zs.values / 9.81 # gravity m/s2 # single_level variables # wind and pressure (no transformations) era[\"u10\"] = ds[\"u10\"].values era[\"v10\"] = ds[\"v10\"].values era[\"ps\"] = ds[\"sp\"].values # temperature variables (degK -> degC) era[\"Td2\"] = ds[\"d2m\"].values - 273.15 era[\"T2\"] = ds[\"t2m\"].values - 273.15 # radiation variables (/sec -> /hour) era[\"SW\"] = ds[\"ssrd\"].values / 3600 era[\"LW\"] = ds[\"strd\"].values / 3600 era[\"S_TOA\"] = ds[\"tisr\"].values / 3600 # precipitation (m -> mm) era[\"P\"] = ds[\"tp\"].values * 1000 # pressure levels era[\"T\"] = ds[\"t\"].values - 273.15 # K to C era[\"Z\"] = ds[\"z\"].values / 9.81 # gravity m/s2 era[\"q\"] = ds[\"q\"].values era[\"u\"] = ds[\"u\"].values era[\"v\"] = ds[\"v\"].values # scaling factors era[\"wind_sf\"] = 1e-2 era[\"q_sf\"] = 1e-6 era[\"ps_sf\"] = 1e2 era[\"rad_sf\"] = 1e-1 era[\"T_sf\"] = 1e-2 era[\"P_sf\"] = 1e-2 # apply scaling factors (done in the original, so we do it here) # wind scaling era[\"u\"] = (era[\"u\"] / era[\"wind_sf\"]).astype(np.int16) era[\"v\"] = (era[\"v\"] / era[\"wind_sf\"]).astype(np.int16) era[\"u10\"] = (era[\"u10\"] / era[\"wind_sf\"]).astype(np.int16) era[\"v10\"] = (era[\"v10\"] / era[\"wind_sf\"]).astype(np.int16) # temperature scaling era[\"T\"] = (era[\"T\"] / era[\"T_sf\"]).astype(np.int16) era[\"Td2\"] = (era[\"Td2\"] / era[\"T_sf\"]).astype(np.int16) era[\"T2\"] = (era[\"T2\"] / era[\"T_sf\"]).astype(np.int16) # humidity scaling era[\"q\"] = (era[\"q\"] / era[\"q_sf\"]).astype(np.uint16) # pressure scaling era[\"ps\"] = (era[\"ps\"] / era[\"ps_sf\"]).astype(np.uint16) # radiation scaling era[\"SW\"] = (era[\"SW\"] / era[\"rad_sf\"]).astype(np.uint16) era[\"LW\"] = (era[\"LW\"] / era[\"rad_sf\"]).astype(np.uint16) era[\"S_TOA\"] = (era[\"S_TOA\"] / era[\"rad_sf\"]).astype(np.uint16) # precipitation scaling era[\"P\"] = (era[\"P\"] / era[\"P_sf\"]).astype(np.uint16) # no scaling for geoportential height era[\"Z\"] = era[\"Z\"].astype(np.int16) out = {\"era\": era} if save_path is not None and isinstance(save_path, str): from scipy.io import savemat savemat(save_path, out, appendmat=True, do_compression=True) return out","title":"Returns"},{"location":"api-reference/#configuration-functions","text":"","title":"Configuration Functions"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel","text":"A class to read CryoGrid Excel configuration files and extract file paths and maybe in the future do some checks etc Source code in cryogrid_pytools/excel_config.py class CryoGridConfigExcel: \"\"\" A class to read CryoGrid Excel configuration files and extract file paths and maybe in the future do some checks etc \"\"\" def __init__(self, fname_xls: str, check_file_paths=True, check_strat_layers=True): \"\"\" Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach. Parameters ---------- fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible \"\"\" self.fname = pathlib.Path(fname_xls).resolve() self.root = self._get_root_path() self._df = self._load_xls(fname_xls) logger.success(f\"Loaded CryoGrid Excel configuration file: {self.fname}\") self.fname = Munch() self.fname.dem = self.get_dem_path() self.fname.coords = self.get_coord_path() self.fname.era5 = self.get_forcing_path() self.fname.datasets = self.get_dataset_paths() self.time = self.get_start_end_times() if check_file_paths: self.check_files_exist() if check_strat_layers: self.check_strat_layers() logger.info( f\"Start and end times: {self.time.time_start:%Y-%m-%d} - {self.time.time_end:%Y-%m-%d}\" ) def _get_root_path(self): \"\"\" Find and set the root path by locating the 'run_cryogrid.m' file. Returns ------- pathlib.Path The discovered root path or the current directory if not found. \"\"\" path = self.fname.parent while True: flist = path.glob(\"run_cryogrid.m\") if len(list(flist)) > 0: self.root = path logger.info(f\"Found root path: {path}\") return self.root elif str(path) == \"/\": logger.warning( \"Could not find root path. Set to current directory. You can change this manually with excel_config.root = pathlib.Path('/path/to/root')\" ) return pathlib.Path(\".\") else: path = path.parent def get_start_end_times(self): \"\"\" Retrieve the start and end times from the Excel configuration. Returns ------- pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. \"\"\" times = self.get_class(\"set_start_end_time\").T.filter(regex=\"time\") times = times.map( lambda x: pd.Timestamp(year=int(x[0]), month=int(x[1]), day=int(x[2])) ) start = times.start_time.min() end = times.end_time.max() times = pd.Series([start, end], index=[\"time_start\", \"time_end\"]) return times def get_coord_path(self): \"\"\" Get the path to the coordinates file from the Excel configuration. Returns ------- pathlib.Path The file path for coordinates. \"\"\" fname = self.get_class_filepath( \"COORDINATES_FROM_FILE\", fname_key=\"file_name\", index=1 ) return fname def get_dataset_paths(self): \"\"\" Retrieve paths for each dataset from the Excel configuration. Returns ------- munch.Munch A dictionary-like object mapping dataset variable names to file paths. \"\"\" paths = ( self.get_class_filepath(\"READ_DATASET\", fname_key=\"filename\") .to_frame(name=\"filepath\") .T ) datasets = self.get_class(\"READ_DATASET\") variable = datasets.T.variable_name paths.loc[\"variable\"] = variable paths = Munch(**paths.T.set_index(\"variable\").filepath.to_dict()) return paths def get_dem_path(self): \"\"\" Get the path for the DEM file from the Excel configuration. Returns ------- pathlib.Path The DEM file path. \"\"\" fname = self.get_class_filepath( \"DEM\", folder_key=\"folder\", fname_key=\"filename\", index=1 ) return fname def get_forcing_path(self, class_name=\"read_mat_ERA\"): \"\"\" Obtain the forcing file path from the Excel configuration. Parameters ---------- class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'. Returns ------- pathlib.Path The forcing file path. \"\"\" fname = self.get_class_filepath( class_name, folder_key=\"path\", fname_key=\"filename\", index=1 ) return fname def get_output_max_depth( self, output_class=\"OUT_regridded_FCI2\", depth_key=\"depth_below_ground\" ) -> int: \"\"\" Get the maximum depth of the output file from the Excel configuration. Parameters ---------- output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'. Returns ------- float The maximum depth value. \"\"\" df = self.get_class(output_class) depth = str(df.loc[depth_key].iloc[0]) depth = int(depth) return depth def check_forcing_fname_times(self): \"\"\" Check if the file name matches the forcing years specified in the Excel configuration. Raises ------ AssertionError If the forcing years in the file name do not match those in the configuration. \"\"\" import re fname = self.get_forcing_path() times = self.get_start_end_times().dt.year.astype(str).values.tolist() fname_years = re.findall(r\"[-_]([12][1089][0-9][0-9])\", fname.stem) assert times == fname_years, ( f\"File name years do not match the forcing years: forcing {times} != fname {fname_years}\" ) def _load_xls(self, fname_xls: str) -> pd.DataFrame: \"\"\" Load the Excel file into a DataFrame. Parameters ---------- fname_xls : str Path to the Excel file. Returns ------- pandas.DataFrame The loaded data with proper indexing and column names. \"\"\" import string alph = list(string.ascii_uppercase) alphabet_extra = alph + [a + b for a in alph for b in alph] df = pd.read_excel(fname_xls, header=None, dtype=str) df.columns = [c for c in alphabet_extra[: df.columns.size]] df.index = df.index + 1 return df def _get_unique_key(self, key: str, col_value=\"B\"): \"\"\" Retrieve a single unique value for a given key from the Excel data. Parameters ---------- key : str The key to look for in column 'A'. col_value : str, optional The column to retrieve the value from, by default 'B'. Returns ------- str or None The found value or None if no value exists. Raises ------ ValueError If multiple values are found for the given key. \"\"\" df = self._df idx = df.A == key value = df.loc[idx, col_value].values if len(value) == 0: return None elif len(value) > 1: raise ValueError(f\"Multiple values found for key: {key}\") else: return value[0] def get_classes(self): \"\"\" Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class(<class_name>). Returns ------- dict of int: str A dictionary mapping class names to row indices \"\"\" df = self._df class_idx = [] for i in range(len(df)): try: self._find_class_block(i) class_idx.append(i) except Exception: pass classes = df.loc[class_idx, \"A\"].to_dict() return classes def get_class_filepath( self, key, folder_key=\"folder\", fname_key=\"file\", index=None ): \"\"\" Construct a file path from folder and file entries in the Excel configuration. Parameters ---------- key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries. Returns ------- pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries. Raises ------ AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. \"\"\" df = self.get_class(key) keys = df.index.values folder_key = keys[[folder_key in k for k in keys]] fname_key = keys[[fname_key in k for k in keys]] assert len(folder_key) == 1, f\"Multiple folder keys found: {folder_key}\" assert len(fname_key) == 1, f\"Multiple fname keys found: {fname_key}\" names = df.loc[[folder_key[0], fname_key[0]]] names = names.apply(lambda ser: self.root / ser.iloc[0] / ser.iloc[1]) if index is None: return names elif isinstance(index, int): return names.loc[f\"{key}_{index}\"] else: raise TypeError(f\"index must be None or int, not {type(index)}\") def get_class(self, class_name: str) -> pd.DataFrame: \"\"\" Return DataFrame blocks representing the specified class from the Excel data. Parameters ---------- class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET'). Returns ------- pandas.DataFrame The concatenated DataFrame of class blocks. \"\"\" df = self._df i0s = df.A == class_name i0s = i0s[i0s].index.values blocks = [self._find_class_block(i0) for i0 in i0s] try: df = pd.concat(blocks, axis=1) except Exception: # only intended for debugging df = blocks logger.warning(f\"Could not concatenate blocks for class: {class_name}\") return df def _find_class_block(self, class_idx0: int): \"\"\" Identify and extract the block of rows corresponding to a class definition. Parameters ---------- class_idx0 : int Starting row index for the class in the Excel data. Returns ------- pandas.DataFrame The processed block as a DataFrame. Raises ------ AssertionError If the class structure is missing required indicators. \"\"\" df = self._df class_name = df.A.loc[class_idx0] msg = f\"Given class_idx0 ({class_name}) is not a class. Must have 'index' adjacent or on cell up and right.\" is_index = df.B.loc[class_idx0 - 1 : class_idx0].str.contains(\"index\") assert is_index.any(), msg index_idx = is_index.idxmax() class_idx0 = index_idx class_idx1 = df.A.loc[class_idx0:] == \"CLASS_END\" # get first True occurrence class_idx1 = class_idx1.idxmax() class_block = df.loc[class_idx0:class_idx1] class_block = self._process_class_block(class_block) return class_block def _process_class_block(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Process a raw class block by removing comments, handling special structures, and shaping data. Parameters ---------- df : pandas.DataFrame A DataFrame slice representing the raw class block. Returns ------- pandas.DataFrame The cleaned and structured DataFrame of class data. Raises ------ AssertionError When matrix structures in the block do not match expected format. \"\"\" \"\"\"hacky way to process the class block\"\"\" # drop CLASS_END row df = df[df.A != \"CLASS_END\"] # if any cell starts with '>', it is a comment df = df.map(lambda x: x if not str(x).startswith(\">\") else np.nan) # drop rows and columns that are all NaN df = df.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\") df = df.astype(str) # H_LIST and V_MATRIX are special cases contains_matrix = df.map(lambda x: \"MATRIX\" in x).values contains_vmatrix = df.map(lambda x: \"V_MATRIX\" in x).values contains_end = df.map(lambda x: \"END\" in x).values ends = np.where(contains_end) if contains_matrix.any(): r0, c0 = [a[0] for a in np.where(contains_matrix)] assert c0 == 1, \"Matrix must be in second column\" assert len(ends) == 2, \"Only two ENDs are allowed\" assert r0 == ends[0][0] assert c0 == ends[1][1] r1 = ends[0][1] c1 = ends[1][0] arr = df.iloc[r0:r1, c0:c1].values if contains_vmatrix.any(): # first column of V_MATRIX is the index but is not in the config file # so we create it. It is one shorter than the num of rows because of header arr[1:, 0] = np.arange(r1 - r0 - 1) matrix = pd.DataFrame(arr[1:, 1:], index=arr[1:, 0], columns=arr[0, 1:]) matrix.index.name = matrix.columns.name = df.iloc[r0, 0] df = df.drop(index=df.index[r0 : r1 + 1]) df.loc[r0, \"A\"] = matrix.index.name df.loc[r0, \"B\"] = (matrix.to_dict(),) for i, row in df.iterrows(): # H_LIST first if row.str.contains(\"H_LIST\").any(): r0 = 2 r1 = row.str.contains(\"END\").argmax() df.loc[i, \"B\"] = row.iloc[r0:r1].values.tolist() class_category = df.A.iloc[0] class_type = df.A.iloc[1] class_index = df.B.iloc[1] col_name = f\"{class_type}_{class_index}\" df = df.iloc[2:, :2].rename(columns=dict(B=col_name)).set_index(\"A\") df.index.name = class_category return df def check_strat_layers(self): \"\"\" Run checks to ensure stratigraphy layers have physically plausible parameter values. \"\"\" strat_layers = self.get_class(\"STRAT_layers\") logger.info(\"Checking stratigraphy layers...\") for layer in strat_layers: try: check_strat_layer_values(strat_layers[layer].iloc[0]) logger.success(f\"[{layer}] parameters passed checks\") except ValueError as error: logger.warning(f\"[{layer}] {error}\") def check_files_exist(self): \"\"\" Check if all the files in the configuration exist. \"\"\" flist = set( [self.get_forcing_path(), self.get_dem_path(), self.get_coord_path()] + list(self.get_dataset_paths().values()) ) logger.info(\"Checking file locations...\") for f in flist: if not f.exists(): logger.warning(f\"Cannot find file: {f}\") else: logger.success(f\"Located file: {f}\")","title":"CryoGridConfigExcel"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.__init__","text":"Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach.","title":"__init__"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.__init__--parameters","text":"fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible Source code in cryogrid_pytools/excel_config.py def __init__(self, fname_xls: str, check_file_paths=True, check_strat_layers=True): \"\"\" Initialize the CryoGridConfigExcel object. Reads in the Excel configuration file and parse the different classes using a pandas DataFrame approach. Parameters ---------- fname_xls : path-like Path to the CryoGrid Excel configuration file. check_file_paths : bool, default=True, optional If True, perform a check that all files linked in the configuration can be found (path exists) check_strat_layers : bool, default=True, optional If True, perform a check that stratigraphy layer parameters are physically plausible \"\"\" self.fname = pathlib.Path(fname_xls).resolve() self.root = self._get_root_path() self._df = self._load_xls(fname_xls) logger.success(f\"Loaded CryoGrid Excel configuration file: {self.fname}\") self.fname = Munch() self.fname.dem = self.get_dem_path() self.fname.coords = self.get_coord_path() self.fname.era5 = self.get_forcing_path() self.fname.datasets = self.get_dataset_paths() self.time = self.get_start_end_times() if check_file_paths: self.check_files_exist() if check_strat_layers: self.check_strat_layers() logger.info( f\"Start and end times: {self.time.time_start:%Y-%m-%d} - {self.time.time_end:%Y-%m-%d}\" )","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.check_files_exist","text":"Check if all the files in the configuration exist. Source code in cryogrid_pytools/excel_config.py def check_files_exist(self): \"\"\" Check if all the files in the configuration exist. \"\"\" flist = set( [self.get_forcing_path(), self.get_dem_path(), self.get_coord_path()] + list(self.get_dataset_paths().values()) ) logger.info(\"Checking file locations...\") for f in flist: if not f.exists(): logger.warning(f\"Cannot find file: {f}\") else: logger.success(f\"Located file: {f}\")","title":"check_files_exist"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.check_forcing_fname_times","text":"Check if the file name matches the forcing years specified in the Excel configuration.","title":"check_forcing_fname_times"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.check_forcing_fname_times--raises","text":"AssertionError If the forcing years in the file name do not match those in the configuration. Source code in cryogrid_pytools/excel_config.py def check_forcing_fname_times(self): \"\"\" Check if the file name matches the forcing years specified in the Excel configuration. Raises ------ AssertionError If the forcing years in the file name do not match those in the configuration. \"\"\" import re fname = self.get_forcing_path() times = self.get_start_end_times().dt.year.astype(str).values.tolist() fname_years = re.findall(r\"[-_]([12][1089][0-9][0-9])\", fname.stem) assert times == fname_years, ( f\"File name years do not match the forcing years: forcing {times} != fname {fname_years}\" )","title":"Raises"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.check_strat_layers","text":"Run checks to ensure stratigraphy layers have physically plausible parameter values. Source code in cryogrid_pytools/excel_config.py def check_strat_layers(self): \"\"\" Run checks to ensure stratigraphy layers have physically plausible parameter values. \"\"\" strat_layers = self.get_class(\"STRAT_layers\") logger.info(\"Checking stratigraphy layers...\") for layer in strat_layers: try: check_strat_layer_values(strat_layers[layer].iloc[0]) logger.success(f\"[{layer}] parameters passed checks\") except ValueError as error: logger.warning(f\"[{layer}] {error}\")","title":"check_strat_layers"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class","text":"Return DataFrame blocks representing the specified class from the Excel data.","title":"get_class"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class--parameters","text":"class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET').","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class--returns","text":"pandas.DataFrame The concatenated DataFrame of class blocks. Source code in cryogrid_pytools/excel_config.py def get_class(self, class_name: str) -> pd.DataFrame: \"\"\" Return DataFrame blocks representing the specified class from the Excel data. Parameters ---------- class_name : str The class name to look up (e.g., 'DEM', 'READ_DATASET'). Returns ------- pandas.DataFrame The concatenated DataFrame of class blocks. \"\"\" df = self._df i0s = df.A == class_name i0s = i0s[i0s].index.values blocks = [self._find_class_block(i0) for i0 in i0s] try: df = pd.concat(blocks, axis=1) except Exception: # only intended for debugging df = blocks logger.warning(f\"Could not concatenate blocks for class: {class_name}\") return df","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class_filepath","text":"Construct a file path from folder and file entries in the Excel configuration.","title":"get_class_filepath"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class_filepath--parameters","text":"key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class_filepath--returns","text":"pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries.","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_class_filepath--raises","text":"AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. Source code in cryogrid_pytools/excel_config.py def get_class_filepath( self, key, folder_key=\"folder\", fname_key=\"file\", index=None ): \"\"\" Construct a file path from folder and file entries in the Excel configuration. Parameters ---------- key : str The class name to search for. folder_key : str, optional Key to identify the folder in the DataFrame, by default 'folder'. fname_key : str, optional Key to identify the file name in the DataFrame, by default 'file'. index : int or None, optional If int, return a single entry. Otherwise return all matched entries. Returns ------- pathlib.Path or pandas.Series The path(s) constructed from the Excel class entries. Raises ------ AssertionError If multiple folder or filename keys are found. TypeError If index is not int or None. \"\"\" df = self.get_class(key) keys = df.index.values folder_key = keys[[folder_key in k for k in keys]] fname_key = keys[[fname_key in k for k in keys]] assert len(folder_key) == 1, f\"Multiple folder keys found: {folder_key}\" assert len(fname_key) == 1, f\"Multiple fname keys found: {fname_key}\" names = df.loc[[folder_key[0], fname_key[0]]] names = names.apply(lambda ser: self.root / ser.iloc[0] / ser.iloc[1]) if index is None: return names elif isinstance(index, int): return names.loc[f\"{key}_{index}\"] else: raise TypeError(f\"index must be None or int, not {type(index)}\")","title":"Raises"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_classes","text":"Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class( ).","title":"get_classes"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_classes--returns","text":"dict of int: str A dictionary mapping class names to row indices Source code in cryogrid_pytools/excel_config.py def get_classes(self): \"\"\" Returns a dictionary of class names and their corresponding row indices from the Excel file. To use as a reference for get_class(<class_name>). Returns ------- dict of int: str A dictionary mapping class names to row indices \"\"\" df = self._df class_idx = [] for i in range(len(df)): try: self._find_class_block(i) class_idx.append(i) except Exception: pass classes = df.loc[class_idx, \"A\"].to_dict() return classes","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_coord_path","text":"Get the path to the coordinates file from the Excel configuration.","title":"get_coord_path"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_coord_path--returns","text":"pathlib.Path The file path for coordinates. Source code in cryogrid_pytools/excel_config.py def get_coord_path(self): \"\"\" Get the path to the coordinates file from the Excel configuration. Returns ------- pathlib.Path The file path for coordinates. \"\"\" fname = self.get_class_filepath( \"COORDINATES_FROM_FILE\", fname_key=\"file_name\", index=1 ) return fname","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_dataset_paths","text":"Retrieve paths for each dataset from the Excel configuration.","title":"get_dataset_paths"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_dataset_paths--returns","text":"munch.Munch A dictionary-like object mapping dataset variable names to file paths. Source code in cryogrid_pytools/excel_config.py def get_dataset_paths(self): \"\"\" Retrieve paths for each dataset from the Excel configuration. Returns ------- munch.Munch A dictionary-like object mapping dataset variable names to file paths. \"\"\" paths = ( self.get_class_filepath(\"READ_DATASET\", fname_key=\"filename\") .to_frame(name=\"filepath\") .T ) datasets = self.get_class(\"READ_DATASET\") variable = datasets.T.variable_name paths.loc[\"variable\"] = variable paths = Munch(**paths.T.set_index(\"variable\").filepath.to_dict()) return paths","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_dem_path","text":"Get the path for the DEM file from the Excel configuration.","title":"get_dem_path"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_dem_path--returns","text":"pathlib.Path The DEM file path. Source code in cryogrid_pytools/excel_config.py def get_dem_path(self): \"\"\" Get the path for the DEM file from the Excel configuration. Returns ------- pathlib.Path The DEM file path. \"\"\" fname = self.get_class_filepath( \"DEM\", folder_key=\"folder\", fname_key=\"filename\", index=1 ) return fname","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_forcing_path","text":"Obtain the forcing file path from the Excel configuration.","title":"get_forcing_path"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_forcing_path--parameters","text":"class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_forcing_path--returns","text":"pathlib.Path The forcing file path. Source code in cryogrid_pytools/excel_config.py def get_forcing_path(self, class_name=\"read_mat_ERA\"): \"\"\" Obtain the forcing file path from the Excel configuration. Parameters ---------- class_name : str, optional The class name to search for in the configuration, by default 'read_mat_ERA'. Returns ------- pathlib.Path The forcing file path. \"\"\" fname = self.get_class_filepath( class_name, folder_key=\"path\", fname_key=\"filename\", index=1 ) return fname","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_output_max_depth","text":"Get the maximum depth of the output file from the Excel configuration.","title":"get_output_max_depth"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_output_max_depth--parameters","text":"output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_output_max_depth--returns","text":"float The maximum depth value. Source code in cryogrid_pytools/excel_config.py def get_output_max_depth( self, output_class=\"OUT_regridded_FCI2\", depth_key=\"depth_below_ground\" ) -> int: \"\"\" Get the maximum depth of the output file from the Excel configuration. Parameters ---------- output_class : str, optional The class name to search for in the configuration, by default 'OUT_regridded_FCI2'. Returns ------- float The maximum depth value. \"\"\" df = self.get_class(output_class) depth = str(df.loc[depth_key].iloc[0]) depth = int(depth) return depth","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_start_end_times","text":"Retrieve the start and end times from the Excel configuration.","title":"get_start_end_times"},{"location":"api-reference/#cryogrid_pytools.excel_config.CryoGridConfigExcel.get_start_end_times--returns","text":"pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. Source code in cryogrid_pytools/excel_config.py def get_start_end_times(self): \"\"\" Retrieve the start and end times from the Excel configuration. Returns ------- pandas.Series A Series with 'time_start' and 'time_end' as Timestamp objects. \"\"\" times = self.get_class(\"set_start_end_time\").T.filter(regex=\"time\") times = times.map( lambda x: pd.Timestamp(year=int(x[0]), month=int(x[1]), day=int(x[2])) ) start = times.start_time.min() end = times.end_time.max() times = pd.Series([start, end], index=[\"time_start\", \"time_end\"]) return times","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values","text":"Validate that stratigraphy layer parameters are physically plausible.","title":"check_strat_layer_values"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values--parameters","text":"tuple_containing_dict : tuple A tuple containing a dictionary whose keys represent layer parameters.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values--raises","text":"ValueError If any parameter check fails for the stratigraphy layers.","title":"Raises"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values--notes","text":"","title":"Notes"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values--definitions","text":"porosity = 1 - mineral - organic airspace = porosity - waterIce volume = mineral + organic + waterIce","title":"Definitions"},{"location":"api-reference/#cryogrid_pytools.excel_config.check_strat_layer_values--checks","text":"field_capacity < porosity : field capacity is a subset of the porosity airspace >= 0 : cannot have negative airspace volume <= 1 : the sum of mineral, organic, and waterIce cannot exceed 1 waterIce <= porosity : waterIce cannot exceed porosity Source code in cryogrid_pytools/excel_config.py def check_strat_layer_values(tuple_containing_dict): \"\"\" Validate that stratigraphy layer parameters are physically plausible. Parameters ---------- tuple_containing_dict : tuple A tuple containing a dictionary whose keys represent layer parameters. Raises ------ ValueError If any parameter check fails for the stratigraphy layers. Notes ----- #### Definitions - `porosity = 1 - mineral - organic` - `airspace = porosity - waterIce` - `volume = mineral + organic + waterIce` #### Checks - `field_capacity < porosity` : field capacity is a subset of the porosity - `airspace >= 0` : cannot have negative airspace - `volume <= 1` : the sum of mineral, organic, and waterIce cannot exceed 1 - `waterIce <= porosity` : waterIce cannot exceed porosity \"\"\" dictionary = tuple_containing_dict[0] df = pd.DataFrame(dictionary).astype(float).round(3) df[\"porosity\"] = (1 - df.mineral - df.organic).round(3) df[\"airspace\"] = (df.porosity - df.waterIce).round(3) df[\"volume\"] = (df.mineral + df.organic + df.waterIce).round(3) checks = pd.DataFrame() checks[\"field_capacity_lt_porosity\"] = df.field_capacity <= df.porosity checks[\"airspace_ge_0\"] = df.airspace >= 0 checks[\"volume_le_1\"] = df.volume <= 1 checks[\"waterice_le_porosity\"] = df.waterIce <= df.porosity checks.index.name = \"layer\" if not checks.values.all(): raise ValueError( \"parameters are not physically plausible. \" \"below are the violations: \\n\" + str(checks.T) )","title":"Checks"},{"location":"api-reference/#data-module-functions","text":"","title":"Data Module Functions"},{"location":"api-reference/#cryogrid_pytools.data.get_dem_copernicus30","text":"Download DEM data from the STAC catalog (default is COP DEM Global 30m).","title":"get_dem_copernicus30"},{"location":"api-reference/#cryogrid_pytools.data.get_dem_copernicus30--parameters","text":"bbox_WSEN : list The bounding box of the area of interest in WSEN format. res_m : int The resolution of the DEM data in meters. epsg : int, optional The EPSG code of the projection of the DEM data. Default is EPSG:32643 (UTM 43N) for the Pamir region. smoothing_iters : int, optional The number of iterations to apply the smoothing filter. Default is 2. Set to 0 to disable smoothing. smoothing_size : int, optional The size of the kernel (num pixels) for the smoothing filter. Default is 3.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.data.get_dem_copernicus30--returns","text":"xarray.DataArray The DEM data as an xarray DataArray with attributes. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_dem_copernicus30( bbox_WSEN: list, res_m: int = 30, epsg=32643, smoothing_iters=2, smoothing_size=3 ) -> _xr.DataArray: \"\"\" Download DEM data from the STAC catalog (default is COP DEM Global 30m). Parameters ---------- bbox_WSEN : list The bounding box of the area of interest in WSEN format. res_m : int The resolution of the DEM data in meters. epsg : int, optional The EPSG code of the projection of the DEM data. Default is EPSG:32643 (UTM 43N) for the Pamir region. smoothing_iters : int, optional The number of iterations to apply the smoothing filter. Default is 2. Set to 0 to disable smoothing. smoothing_size : int, optional The size of the kernel (num pixels) for the smoothing filter. Default is 3. Returns ------- xarray.DataArray The DEM data as an xarray DataArray with attributes. \"\"\" from .utils import drop_coords_without_dim check_epsg(epsg) assert res_m >= 30, ( \"The resolution must be greater than 30m for the COP DEM Global 30m dataset.\" ) res = res_m / 111111 if epsg == 4326 else res_m _logger.info(\"Fetching COP DEM Global 30m data from Planetary Computer\") items = search_stac_items_planetary_computer(\"cop-dem-glo-30\", bbox_WSEN) da_dem = _stackstac.stack( items=items, bounds_latlon=bbox_WSEN, resolution=res, epsg=epsg ) da_dem = ( da_dem.mean(\"time\") .squeeze() .pipe(drop_coords_without_dim) .pipe(smooth_data, n_iters=smoothing_iters, kernel_size=smoothing_size) .rio.write_crs(f\"EPSG:{epsg}\") .assign_attrs( source=items[0].links[0].href, # collection URL bbox_request=bbox_WSEN, ) ) return da_dem","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.data.get_esa_land_cover","text":"Get the ESA World Cover dataset on the target grid and resolution.","title":"get_esa_land_cover"},{"location":"api-reference/#cryogrid_pytools.data.get_esa_land_cover--parameters","text":"bbox_WSEN : tuple Bounding box in the format (West, South, East, North). res_m : int, optional Resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.data.get_esa_land_cover--returns","text":"xr.DataArray A DataArray with the land cover data on the target grid. Contains attributes 'class_values', 'class_descriptions', 'class_colors' for plotting. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_esa_land_cover(bbox_WSEN: tuple, res_m: int = 30, epsg=32643) -> _xr.DataArray: \"\"\" Get the ESA World Cover dataset on the target grid and resolution. Parameters ---------- bbox_WSEN : tuple Bounding box in the format (West, South, East, North). res_m : int, optional Resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns ------- xr.DataArray A DataArray with the land cover data on the target grid. Contains attributes 'class_values', 'class_descriptions', 'class_colors' for plotting. \"\"\" def get_land_cover_classes(item): \"\"\" Get the land cover class names, and colors from the ESA World Cover dataset Args: item (pystac.Item): The STAC item containing the land cover data. Returns: dict: A dictionary with class values, descriptions, and colors. \"\"\" import pandas as pd classes = item.assets[\"map\"].extra_fields[\"classification:classes\"] df = ( pd.DataFrame(classes) .set_index(\"value\") .rename( columns=lambda s: s.replace(\"-\", \"_\") ) # bug fix for version 2.7.8 (stacstack back compatibility) ) df[\"color_hint\"] = \"#\" + df[\"color_hint\"] out = dict( class_values=df.index.values, class_descriptions=df[\"description\"].values, class_colors=df[\"color_hint\"].values, ) return out # make sure epsg is supported check_epsg(epsg) # get the units in the projection res = get_res_in_proj_units(res_m, epsg, min_res=10) _logger.info(\"Fetching ESA World Cover (v2.0) data from Planetary Computer\") items = search_stac_items_planetary_computer( collection=\"esa-worldcover\", bbox=bbox_WSEN, query={\"esa_worldcover:product_version\": {\"eq\": \"2.0.0\"}}, ) stac_props = dict( items=items, assets=[\"map\"], epsg=epsg, bounds_latlon=bbox_WSEN, resolution=res ) da = ( _stackstac.stack(**stac_props) .max([\"band\", \"time\"], keep_attrs=True) # removing the single band dimension .rename(\"land_cover\") .assign_attrs(**get_land_cover_classes(items[0])) ) return da","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.data.get_snow_melt_doy","text":"Calculate the snow melt day of year (DOY) from Sentinel-2 SCL data for a given bounding box and years.","title":"get_snow_melt_doy"},{"location":"api-reference/#cryogrid_pytools.data.get_snow_melt_doy--parameters","text":"bbox_WSEN : tuple Bounding box coordinates in the format (West, South, East, North). years : range, optional Range of years to consider. Defaults to range(2018, 2025). res_m : int, optional Spatial resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643.","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.data.get_snow_melt_doy--returns","text":"_xr.DataArray DataArray containing the snow melt DOY for each year. Source code in cryogrid_pytools/data.py @_decorator_dataarray_to_bbox def get_snow_melt_doy( bbox_WSEN: tuple, years=range(2018, 2025), res_m: int = 30, epsg=32643 ) -> _xr.DataArray: \"\"\" Calculate the snow melt day of year (DOY) from Sentinel-2 SCL data for a given bounding box and years. Parameters ---------- bbox_WSEN : tuple Bounding box coordinates in the format (West, South, East, North). years : range, optional Range of years to consider. Defaults to range(2018, 2025). res_m : int, optional Spatial resolution in meters. Defaults to 30. epsg : int, optional EPSG code for the coordinate reference system. Defaults to 32643. Returns ------- _xr.DataArray DataArray containing the snow melt DOY for each year. \"\"\" da = get_sentinel2_data( bbox_WSEN, years=years, res_m=res_m, epsg=epsg, max_cloud_cover=10 ) _logger.info(\"Calculating snow melt day of year (DOY) from Sentinel-2 SCL data\") doy = da.groupby(\"time.year\").apply(calc_sentinel2_snow_melt_doy) return doy","title":"Returns"},{"location":"api-reference/#cryogrid_pytools.data.get_randolph_glacier_inventory","text":"Fetches the Randolph Glacier Inventory (RGI) data and returns it as a GeoDataFrame or raster dataset.","title":"get_randolph_glacier_inventory"},{"location":"api-reference/#cryogrid_pytools.data.get_randolph_glacier_inventory--parameters","text":"target_dem : optional A digital elevation model (DEM) object. If provided, the function will return the RGI data clipped to the bounding box of the DEM and reprojected to the DEM's CRS. dest_dir : str, optional The directory where the downloaded RGI data will be stored. If None, the data will be stored in the pooch cache directory (~/.cache/pooch/).","title":"Parameters"},{"location":"api-reference/#cryogrid_pytools.data.get_randolph_glacier_inventory--returns","text":"GeoDataFrame or raster dataset If target_dem is None, returns a GeoDataFrame containing the RGI data. If target_dem is provided, returns a raster dataset clipped and reprojected to the DEM. Source code in cryogrid_pytools/data.py @_cached def get_randolph_glacier_inventory(target_dem=None, dest_dir=None): \"\"\" Fetches the Randolph Glacier Inventory (RGI) data and returns it as a GeoDataFrame or raster dataset. Parameters ---------- target_dem : optional A digital elevation model (DEM) object. If provided, the function will return the RGI data clipped to the bounding box of the DEM and reprojected to the DEM's CRS. dest_dir : str, optional The directory where the downloaded RGI data will be stored. If None, the data will be stored in the pooch cache directory (~/.cache/pooch/). Returns ------- GeoDataFrame or raster dataset If target_dem is None, returns a GeoDataFrame containing the RGI data. If target_dem is provided, returns a raster dataset clipped and reprojected to the DEM. \"\"\" url = \"https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0770_rgi_v7/regional_files/RGI2000-v7.0-G/RGI2000-v7.0-G-13_central_asia.zip\" downloader = _pooch.HTTPDownloader( progressbar=True, headers=get_earthaccess_session().headers ) flist = download_url(url, path=dest_dir, downloader=downloader) fname_shp = [f for f in flist if f.endswith(\".shp\")][0] _logger.log( \"INFO\", \"RGI: Fetching Randolph Glacier Inventory - see https://www.glims.org/rgi_user_guide/welcome.html\", ) _logger.log(\"DEBUG\", f\"RGI: URL = {url}\") _logger.log(\"DEBUG\", f\"RGI: FILE = {fname_shp}\") if target_dem is None: # reads the whole file df = _gpd.read_file(fname_shp) else: # gets the bounding box and then reads the file bbox = target_dem.rv.get_bbox_latlon() df = _gpd.read_file(fname_shp, bbox=bbox).to_crs(target_dem.rio.crs) df = df.dissolve() ds = df.rv.to_raster(target_dem) return ds return df","title":"Returns"},{"location":"api-reference/#utility-functions","text":"Additional utility functions are available in the package. See the source code documentation for more details.","title":"Utility Functions"},{"location":"installation/","text":"Installation CryoGrid-pyTools can be easily installed using pip: pip install cryogrid_pytools Optional Dependencies CryoGrid-pyTools has several optional dependency groups that can be installed based on your needs: Documentation Dependencies To build the documentation locally, install with the docs extra: pip install \"cryogrid_pytools[docs]\" Data Processing Dependencies For additional data processing capabilities, install with the data extra: pip install \"cryogrid_pytools[data]\" Development Installation If you want to contribute to the development of CryoGrid-pyTools, you can install from source: git clone https://github.com/lukegre/CryoGrid-pyTools.git cd CryoGrid-pyTools pip install -e \".[docs,data]\" # install with all optional dependencies Requirements CryoGrid-pyTools requires Python 3.9 or later. The main dependencies are: numpy >= 2.0 scipy >= 1.13.1 xarray >= 2024 pandas >= 2 dask[array,diagnostics] >= 2024 These dependencies will be automatically installed when you install the package using pip.","title":"Installation"},{"location":"installation/#installation","text":"CryoGrid-pyTools can be easily installed using pip: pip install cryogrid_pytools","title":"Installation"},{"location":"installation/#optional-dependencies","text":"CryoGrid-pyTools has several optional dependency groups that can be installed based on your needs:","title":"Optional Dependencies"},{"location":"installation/#documentation-dependencies","text":"To build the documentation locally, install with the docs extra: pip install \"cryogrid_pytools[docs]\"","title":"Documentation Dependencies"},{"location":"installation/#data-processing-dependencies","text":"For additional data processing capabilities, install with the data extra: pip install \"cryogrid_pytools[data]\"","title":"Data Processing Dependencies"},{"location":"installation/#development-installation","text":"If you want to contribute to the development of CryoGrid-pyTools, you can install from source: git clone https://github.com/lukegre/CryoGrid-pyTools.git cd CryoGrid-pyTools pip install -e \".[docs,data]\" # install with all optional dependencies","title":"Development Installation"},{"location":"installation/#requirements","text":"CryoGrid-pyTools requires Python 3.9 or later. The main dependencies are: numpy >= 2.0 scipy >= 1.13.1 xarray >= 2024 pandas >= 2 dask[array,diagnostics] >= 2024 These dependencies will be automatically installed when you install the package using pip.","title":"Requirements"},{"location":"usage/forcing/","text":"Working with Forcing Data CryoGrid-pyTools provides functionality to work with ERA5 forcing data for CryoGrid simulations. Reading ERA5 Data You can read ERA5 forcing data from MATLAB files using the read_mat_ear5 function: import cryogrid_pytools as cg # Read ERA5 forcing data ds = cg.read_mat_ear5('path/to/ERA5.mat') The returned xarray Dataset contains all ERA5 forcing variables needed for CryoGrid simulations. Converting ERA5 Data If you have ERA5 data from the Copernicus Climate Data Store (CDS) in netCDF format, you can convert it to the format expected by CryoGrid using era5_to_matlab : import cryogrid_pytools as cg # Convert ERA5 netCDF to MATLAB format ds = cg.era5_to_matlab( era5_dataset, # xarray Dataset from ERA5 CDS save_path='ERA5.mat' # Optional: save to MATLAB file ) The function expects the following variables in the input dataset: Single Level Variables u10, v10: 10m wind components sp: Surface pressure d2m: 2m dewpoint temperature t2m: 2m temperature ssrd: Surface solar radiation downwards strd: Surface thermal radiation downwards tisr: TOA incident solar radiation tp: Total precipitation Zs: Surface geopotential (static) Pressure Level Variables t: Temperature z: Geopotential q: Specific humidity u, v: Wind components The output will be formatted to match the requirements of the CryoGrid.POST_PROC.read_mat_ERA class in MATLAB.","title":"Working with Forcing Data"},{"location":"usage/forcing/#working-with-forcing-data","text":"CryoGrid-pyTools provides functionality to work with ERA5 forcing data for CryoGrid simulations.","title":"Working with Forcing Data"},{"location":"usage/forcing/#reading-era5-data","text":"You can read ERA5 forcing data from MATLAB files using the read_mat_ear5 function: import cryogrid_pytools as cg # Read ERA5 forcing data ds = cg.read_mat_ear5('path/to/ERA5.mat') The returned xarray Dataset contains all ERA5 forcing variables needed for CryoGrid simulations.","title":"Reading ERA5 Data"},{"location":"usage/forcing/#converting-era5-data","text":"If you have ERA5 data from the Copernicus Climate Data Store (CDS) in netCDF format, you can convert it to the format expected by CryoGrid using era5_to_matlab : import cryogrid_pytools as cg # Convert ERA5 netCDF to MATLAB format ds = cg.era5_to_matlab( era5_dataset, # xarray Dataset from ERA5 CDS save_path='ERA5.mat' # Optional: save to MATLAB file ) The function expects the following variables in the input dataset:","title":"Converting ERA5 Data"},{"location":"usage/forcing/#single-level-variables","text":"u10, v10: 10m wind components sp: Surface pressure d2m: 2m dewpoint temperature t2m: 2m temperature ssrd: Surface solar radiation downwards strd: Surface thermal radiation downwards tisr: TOA incident solar radiation tp: Total precipitation Zs: Surface geopotential (static)","title":"Single Level Variables"},{"location":"usage/forcing/#pressure-level-variables","text":"t: Temperature z: Geopotential q: Specific humidity u, v: Wind components The output will be formatted to match the requirements of the CryoGrid.POST_PROC.read_mat_ERA class in MATLAB.","title":"Pressure Level Variables"},{"location":"usage/getting-started/","text":"Getting Started This guide will help you get started with CryoGrid-pyTools. The package provides various functions to work with CryoGrid data in Python. Basic Usage First, import the package: import cryogrid_pytools as cg The package provides several main functionalities: Reading CryoGrid output files Processing MATLAB struct files Working with ERA5 forcing data Handling configuration files Check out the specific guides for each functionality: Reading CryoGrid Output Reading MATLAB Structs Example Notebook For comprehensive examples, check out the demo.ipynb notebook in the repository. It contains detailed examples of various use cases and functionalities.","title":"Getting Started"},{"location":"usage/getting-started/#getting-started","text":"This guide will help you get started with CryoGrid-pyTools. The package provides various functions to work with CryoGrid data in Python.","title":"Getting Started"},{"location":"usage/getting-started/#basic-usage","text":"First, import the package: import cryogrid_pytools as cg The package provides several main functionalities: Reading CryoGrid output files Processing MATLAB struct files Working with ERA5 forcing data Handling configuration files Check out the specific guides for each functionality: Reading CryoGrid Output Reading MATLAB Structs","title":"Basic Usage"},{"location":"usage/getting-started/#example-notebook","text":"For comprehensive examples, check out the demo.ipynb notebook in the repository. It contains detailed examples of various use cases and functionalities.","title":"Example Notebook"},{"location":"usage/reading-matlab/","text":"Reading MATLAB Structs CryoGrid-pyTools provides functionality to read MATLAB struct files into Python. Here's how to work with MATLAB data. Important Note Warning The run_info.mat file cannot be read directly as it contains special classes not supported by scipy.io.loadmat . You'll need to save the required data in a different format from MATLAB. Preparing MATLAB Data When working in MATLAB, ensure you: Add the CryoGrid/source directory to the MATLAB path before saving files Save data in a compatible format For example, to save parts of run_info , use this MATLAB code: % Save specific variables from run_info save('my_data.mat', 'variable1', 'variable2', '-v7.3') Reading MATLAB Files in Python Once you have your MATLAB data in a compatible format: import cryogrid_pytools as cg # Read the MATLAB file data = cg.read_matlab_file('my_data.mat') The data will be converted to appropriate Python data structures, making it easy to work with in your Python environment.","title":"Reading MATLAB Structs"},{"location":"usage/reading-matlab/#reading-matlab-structs","text":"CryoGrid-pyTools provides functionality to read MATLAB struct files into Python. Here's how to work with MATLAB data.","title":"Reading MATLAB Structs"},{"location":"usage/reading-matlab/#important-note","text":"Warning The run_info.mat file cannot be read directly as it contains special classes not supported by scipy.io.loadmat . You'll need to save the required data in a different format from MATLAB.","title":"Important Note"},{"location":"usage/reading-matlab/#preparing-matlab-data","text":"When working in MATLAB, ensure you: Add the CryoGrid/source directory to the MATLAB path before saving files Save data in a compatible format For example, to save parts of run_info , use this MATLAB code: % Save specific variables from run_info save('my_data.mat', 'variable1', 'variable2', '-v7.3')","title":"Preparing MATLAB Data"},{"location":"usage/reading-matlab/#reading-matlab-files-in-python","text":"Once you have your MATLAB data in a compatible format: import cryogrid_pytools as cg # Read the MATLAB file data = cg.read_matlab_file('my_data.mat') The data will be converted to appropriate Python data structures, making it easy to work with in your Python environment.","title":"Reading MATLAB Files in Python"},{"location":"usage/reading-output/","text":"Reading CryoGrid Output CryoGrid-pyTools provides functionality to read CryoGrid output files into Python. Currently, it supports reading regridded FCI2 output files. Reading FCI2 Output Files Use the read_OUT_regridded_FCI2_file function to read a regridded FCI2 output file: import cryogrid_pytools as cg # Read a single output file ds = cg.read_OUT_regridded_FCI2_file( 'path/to/your/output_file.mat', deepest_point=-5 # Set the deepest point in meters ) The function returns an xarray Dataset with the following variables: - time : Time coordinate (datetime64) - depth : Depth coordinate in meters - T : Temperature - water : Water content - ice : Ice content - class_number : Class number - FCI : Frozen/Thawed state - elevation : Surface elevation Reading Multiple Files For spatial runs with multiple output files, use read_OUT_regridded_FCI2_clusters : # Read multiple output files ds = cg.read_OUT_regridded_FCI2_clusters( 'path/to/output/directory/*.mat', # Glob pattern for output files deepest_point=-5 ) The resulting Dataset will have an additional gridcell dimension for the spatial component. Data Structure The output is an xarray Dataset with: Dimensions: time : Timesteps in the simulation depth : Vertical grid points gridcell : (Only for cluster runs) Spatial grid points Variables: All variables are stored as dask arrays for efficient memory usage Temperature and other fields are stored with dimensions (time, depth) or (gridcell, depth, time) Working with the Data Being an xarray Dataset, you can use all standard xarray operations: ```python Select a specific time ds.sel(time='2000-01-01') Get mean temperature over time ds.T.mean(dim='time') Plot temperature profile ds.T.plot()","title":"Reading CryoGrid Output"},{"location":"usage/reading-output/#reading-cryogrid-output","text":"CryoGrid-pyTools provides functionality to read CryoGrid output files into Python. Currently, it supports reading regridded FCI2 output files.","title":"Reading CryoGrid Output"},{"location":"usage/reading-output/#reading-fci2-output-files","text":"Use the read_OUT_regridded_FCI2_file function to read a regridded FCI2 output file: import cryogrid_pytools as cg # Read a single output file ds = cg.read_OUT_regridded_FCI2_file( 'path/to/your/output_file.mat', deepest_point=-5 # Set the deepest point in meters ) The function returns an xarray Dataset with the following variables: - time : Time coordinate (datetime64) - depth : Depth coordinate in meters - T : Temperature - water : Water content - ice : Ice content - class_number : Class number - FCI : Frozen/Thawed state - elevation : Surface elevation","title":"Reading FCI2 Output Files"},{"location":"usage/reading-output/#reading-multiple-files","text":"For spatial runs with multiple output files, use read_OUT_regridded_FCI2_clusters : # Read multiple output files ds = cg.read_OUT_regridded_FCI2_clusters( 'path/to/output/directory/*.mat', # Glob pattern for output files deepest_point=-5 ) The resulting Dataset will have an additional gridcell dimension for the spatial component.","title":"Reading Multiple Files"},{"location":"usage/reading-output/#data-structure","text":"The output is an xarray Dataset with: Dimensions: time : Timesteps in the simulation depth : Vertical grid points gridcell : (Only for cluster runs) Spatial grid points Variables: All variables are stored as dask arrays for efficient memory usage Temperature and other fields are stored with dimensions (time, depth) or (gridcell, depth, time)","title":"Data Structure"},{"location":"usage/reading-output/#working-with-the-data","text":"Being an xarray Dataset, you can use all standard xarray operations: ```python","title":"Working with the Data"},{"location":"usage/reading-output/#select-a-specific-time","text":"ds.sel(time='2000-01-01')","title":"Select a specific time"},{"location":"usage/reading-output/#get-mean-temperature-over-time","text":"ds.T.mean(dim='time')","title":"Get mean temperature over time"},{"location":"usage/reading-output/#plot-temperature-profile","text":"ds.T.plot()","title":"Plot temperature profile"},{"location":"usage/spatial-data/","text":"Spatial Data and Forcing The data module provides tools for creating forcing data and spatial data for CryoGrid spatial cluster runs. This module requires additional dependencies which can be installed using: pip install \"cryogrid_pytools[data]\" Digital Elevation Model (DEM) You can download DEM data from the Copernicus 30m dataset: import cryogrid_pytools as cg # Define your area of interest (West, South, East, North) bbox = [70.0, 35.0, 71.0, 36.0] # Example for a region in the Pamirs # Get DEM data at 30m resolution dem = cg.data.get_dem_copernicus30( bbox_WSEN=bbox, res_m=30, epsg=32643, # UTM 43N (default for Pamir region) smoothing_iters=2, # Apply smoothing to reduce noise smoothing_size=3 # Kernel size for smoothing ) Land Cover Data Get ESA World Cover data for your region: landcover = cg.data.get_esa_land_cover( bbox_WSEN=bbox, res_m=30, epsg=32643 ) The returned DataArray includes attributes for class values, descriptions, and colors that can be used for plotting. Snow Melt Timing Calculate snow melt timing using Sentinel-2 data: # Get snow melt day of year for multiple years snow_melt = cg.data.get_snow_melt_doy( bbox_WSEN=bbox, years=range(2018, 2025), # Analysis period res_m=30, epsg=32643 ) Glacier Data Get Randolph Glacier Inventory (RGI) data for your region: # Get glacier data as a raster matching your DEM glacier_data = cg.data.get_randolph_glacier_inventory(target_dem=dem) # Or get raw vector data glacier_vector = cg.data.get_randolph_glacier_inventory() ERA5 Forcing Data The module provides access to ERA5 climate forcing data through the era5_downloader package: from cryogrid_pytools.data import make_era5_downloader # Create an ERA5 downloader instance era5 = make_era5_downloader() # Download ERA5 data for your region and time period forcing = era5.get_data( bbox_WSEN=bbox, start_date=\"2018-01-01\", end_date=\"2024-12-31\" ) Advanced Usage Smoothing Data You can smooth any spatial data using a rolling mean filter: smoothed_dem = cg.data.smooth_data( dem, kernel_size=3, n_iters=2 ) Working with Sentinel-2 Data Get raw Sentinel-2 data for custom analysis: sentinel_data = cg.data.get_sentinel2_data( bbox_WSEN=bbox, years=range(2018, 2025), assets=['SCL'], # Scene Classification Layer res_m=30, epsg=32643, max_cloud_cover=5 # Maximum cloud cover percentage ) Notes All spatial functions support consistent coordinate reference systems through the epsg parameter Resolution can be specified in meters using the res_m parameter The module handles data downloads and caching automatically Most functions support both vector (GeoDataFrame) and raster (xarray.DataArray) outputs Functions are decorated to handle both bounding box inputs and existing DataArrays for reprojection","title":"Spatial Data and Forcing"},{"location":"usage/spatial-data/#spatial-data-and-forcing","text":"The data module provides tools for creating forcing data and spatial data for CryoGrid spatial cluster runs. This module requires additional dependencies which can be installed using: pip install \"cryogrid_pytools[data]\"","title":"Spatial Data and Forcing"},{"location":"usage/spatial-data/#digital-elevation-model-dem","text":"You can download DEM data from the Copernicus 30m dataset: import cryogrid_pytools as cg # Define your area of interest (West, South, East, North) bbox = [70.0, 35.0, 71.0, 36.0] # Example for a region in the Pamirs # Get DEM data at 30m resolution dem = cg.data.get_dem_copernicus30( bbox_WSEN=bbox, res_m=30, epsg=32643, # UTM 43N (default for Pamir region) smoothing_iters=2, # Apply smoothing to reduce noise smoothing_size=3 # Kernel size for smoothing )","title":"Digital Elevation Model (DEM)"},{"location":"usage/spatial-data/#land-cover-data","text":"Get ESA World Cover data for your region: landcover = cg.data.get_esa_land_cover( bbox_WSEN=bbox, res_m=30, epsg=32643 ) The returned DataArray includes attributes for class values, descriptions, and colors that can be used for plotting.","title":"Land Cover Data"},{"location":"usage/spatial-data/#snow-melt-timing","text":"Calculate snow melt timing using Sentinel-2 data: # Get snow melt day of year for multiple years snow_melt = cg.data.get_snow_melt_doy( bbox_WSEN=bbox, years=range(2018, 2025), # Analysis period res_m=30, epsg=32643 )","title":"Snow Melt Timing"},{"location":"usage/spatial-data/#glacier-data","text":"Get Randolph Glacier Inventory (RGI) data for your region: # Get glacier data as a raster matching your DEM glacier_data = cg.data.get_randolph_glacier_inventory(target_dem=dem) # Or get raw vector data glacier_vector = cg.data.get_randolph_glacier_inventory()","title":"Glacier Data"},{"location":"usage/spatial-data/#era5-forcing-data","text":"The module provides access to ERA5 climate forcing data through the era5_downloader package: from cryogrid_pytools.data import make_era5_downloader # Create an ERA5 downloader instance era5 = make_era5_downloader() # Download ERA5 data for your region and time period forcing = era5.get_data( bbox_WSEN=bbox, start_date=\"2018-01-01\", end_date=\"2024-12-31\" )","title":"ERA5 Forcing Data"},{"location":"usage/spatial-data/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"usage/spatial-data/#smoothing-data","text":"You can smooth any spatial data using a rolling mean filter: smoothed_dem = cg.data.smooth_data( dem, kernel_size=3, n_iters=2 )","title":"Smoothing Data"},{"location":"usage/spatial-data/#working-with-sentinel-2-data","text":"Get raw Sentinel-2 data for custom analysis: sentinel_data = cg.data.get_sentinel2_data( bbox_WSEN=bbox, years=range(2018, 2025), assets=['SCL'], # Scene Classification Layer res_m=30, epsg=32643, max_cloud_cover=5 # Maximum cloud cover percentage )","title":"Working with Sentinel-2 Data"},{"location":"usage/spatial-data/#notes","text":"All spatial functions support consistent coordinate reference systems through the epsg parameter Resolution can be specified in meters using the res_m parameter The module handles data downloads and caching automatically Most functions support both vector (GeoDataFrame) and raster (xarray.DataArray) outputs Functions are decorated to handle both bounding box inputs and existing DataArrays for reprojection","title":"Notes"}]}